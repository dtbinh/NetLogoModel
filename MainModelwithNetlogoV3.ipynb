{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "# version 2: did not use imputation. just ML directly \n",
    "\n",
    "# parameters:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from IPython.display import Image\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from dateutil.parser import parse\n",
    "#from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute\n",
    "from sklearn.preprocessing import StandardScaler                 #normalising features\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "#from sknn import mlp\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py4j.java_gateway import JavaGateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gw = JavaGateway() # New gateway connection\n",
    "bridge = gw.entry_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load netlogo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the model:\n",
    "models_path = \"D:/Work/PhD/Github/NetLogoModel/\"\n",
    "model_name = \"PhD_InitialModelV3.nlogo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelLocation = models_path + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Work/PhD/Github/NetLogoModel/PhD_InitialModelV3.nlogo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bridge.openModel(modelLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run one iteration of the model, one command at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bridge.command(\"set grid-size 21\")\n",
    "bridge.command(\"set nb-people 1\")\n",
    "bridge.command(\"set Show_Names_Nodes? False\")\n",
    "bridge.command(\"set Show_Names_people? False\")\n",
    "bridge.command(\"set car-speed 1\")\n",
    "bridge.command(\"set mobiletowerdata? True\")\n",
    "bridge.command(\"set socialmedia_data? True\")\n",
    "bridge.command(\"set grids_covered_vector 10\")\n",
    "bridge.command(\"set avg_num_calls_perday 100\")\n",
    "bridge.command(\"set avg_call_duration_mins 2\")\n",
    "\n",
    "bridge.command(\"random-seed 0\")\n",
    "bridge.command(\"setup\")\n",
    "bridge.command(\"repeat 1440 [go]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bridge.command(\"repeat 144000 [go]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Have the model report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 15 ./agentdata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 15 ./mobiletowers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiating the netlogo model\n",
    "# and run it for the first 10 days\n",
    "def model_init(ini_mins):\n",
    "    bridge.command(\"set zones-x 2\")\n",
    "    bridge.command(\"set zones-y 2\")\n",
    "    bridge.command(\"set grid-size 21\")\n",
    "    bridge.command(\"set nb-people 1\")\n",
    "    bridge.command(\"set Show_Names_Nodes? False\")\n",
    "    bridge.command(\"set Show_Names_people? False\")\n",
    "    bridge.command(\"set car-speed 1\")\n",
    "    bridge.command(\"set mobiletowerdata? True\")\n",
    "    bridge.command(\"set socialmedia_data? True\")\n",
    "    bridge.command(\"set grids_covered_vector 5\")\n",
    "    bridge.command(\"set avg_num_calls_perday 10\")            #setting for the usage of mobile phones\n",
    "    bridge.command(\"set avg_call_duration_mins 2\")\n",
    "    bridge.command(\"set avg_num_sm_usage 5\")                 #setting for the usage of social media\n",
    "    bridge.command(\"random-seed 10\")\n",
    "    bridge.command(\"setup\")\n",
    "    bridge.command(\"repeat \" + str(ini_mins) + \" [go]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    #\"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in xrange(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "def population_gen(num_pop, num_zone, num_act):\n",
    "    # generating population, their attributes, locations for home, work, shop, school etc.\n",
    "    loc = []\n",
    "    poploc = [None]*num_pop\n",
    "    zonerange = np.arange(1, 0.1*num_zone + 1, 0.1)\n",
    "    # divide equally between the zones based on the number of activity locations\n",
    "    # then choose randomly from each split \n",
    "    zonerange = list(chunks(zonerange, num_zone/num_act))\n",
    "    print zonerange\n",
    "    for j in range(num_pop):\n",
    "        for i in range(num_act):\n",
    "            loc = np.hstack((loc, round(random.choice(zonerange[i]),2)))\n",
    "        poploc[j] = loc\n",
    "        loc = []\n",
    "    \n",
    "    pd.DataFrame(poploc).to_csv('population.csv', header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Schedule_Gen(population, CV, day_num):\n",
    "    # generate population and schedules\n",
    "    if CV == 0:\n",
    "        CV = 0.000001\n",
    "    pop_all = pd.read_csv('population.csv', header=None)\n",
    "    pop_num = pop_all[0]\n",
    "    type_num = 1    # type of the person being generated: 1: full time employment\n",
    "    #day_num = 100   # number of days to be simulated\n",
    "\n",
    "    # assuming the following schedule sequence:\n",
    "    mu_Wmorning = 450 #time to go out (7:30)\n",
    "    sigma_Wmorning = mu_Wmorning*CV #standard deviation of morning trip\n",
    "    mu_Wnoon = 210 #duration of morning work\n",
    "    sigma_Wnoon = mu_Wnoon*CV #standard deviation of the duration of morning work\n",
    "    mu_L = 60 # duration of lunch break\n",
    "    sigma_L = mu_L*CV #standard deviation of the duration of lunch break\n",
    "    mu_Wafnoon = 270 #duration of afternoon work\n",
    "    sigma_Wafnoon = mu_Wafnoon*CV #standard deviation of the duration of morning work\n",
    "\n",
    "    trip_Wmorning = [int(x) for x in np.random.normal(mu_Wmorning, sigma_Wmorning, day_num)] #generate start time based on normal distribution\n",
    "    trip_Wnoon = [int(x) for x in np.random.normal(mu_Wnoon, sigma_Wnoon, day_num)] # work duration in the morning\n",
    "    trip_Wafnoon = [int(x) for x in np.random.normal(mu_Wafnoon, sigma_Wafnoon, day_num)] #work duration in the afternoon\n",
    "    trip_L = [int(x) for x in np.random.normal(mu_L, sigma_L, day_num)] #lunch break in the noon\n",
    "    trip_Smorning = [int(x) for x in np.random.normal(mu_L, sigma_L, day_num)] #school trip in the morning\n",
    "    trip_Safnoon = [int(x) for x in np.random.normal(mu_L, sigma_L, day_num)] #school trip in the afternoon\n",
    "\n",
    "    for y in pop_num:\n",
    "        cur_person = pop_all.iloc[y,:]\n",
    "        z_home = cur_person[1] ##zone number for home\n",
    "        z_work = cur_person[2] ##zone for work\n",
    "        z_shop = cur_person[3] ##zone number for shopping\n",
    "        z_school = cur_person[4] ##zone number for school\n",
    "        demand_data = pd.DataFrame(None)\n",
    "        demand_data['id'] = range(day_num*1440) \n",
    "        demand_data['day_id'] = [int(x/ 1440) for x in demand_data['id']]\n",
    "        demand_data['minute_id'] = demand_data['id'] % 1440\n",
    "        base = datetime.datetime.strptime('2015-01-01 00:00',  '%Y-%m-%d %H:%M')\n",
    "        datetime_list = [base + datetime.timedelta(minutes=x) for x in range(0, day_num*1440)]\n",
    "        dow = [datetime.datetime.isoweekday(x) for x in datetime_list]\n",
    "        hr = [x.hour for x in datetime_list]\n",
    "        minu = [x.minute for x in datetime_list]\n",
    "        demand_data['datetime'] = [datetime.datetime.strftime(x, '%Y-%m-%d %H:%M') for x in datetime_list]\n",
    "        demand_data['dow'] = dow\n",
    "        demand_data['hour'] = hr\n",
    "        demand_data['minute'] = minu\n",
    "        demand_data['pid'] = cur_person[0]\n",
    "        demand_data['type'] = type_num\n",
    "        demand_data['zone'] = None\n",
    "        cur_day = None\n",
    "        uni_day = np.unique(demand_data['day_id'])\n",
    "        for i in uni_day:\n",
    "            cur_min = demand_data['minute_id'][i]\n",
    "            Sch_drop_S = trip_Wmorning[i]\n",
    "            Sch_drop_E = trip_Wmorning[i] + trip_Smorning[i]\n",
    "            Wrk_Mor_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i]\n",
    "            Shp_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i] + trip_L[i]\n",
    "            Wrk_Aft_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i] + trip_L[i] + trip_Wafnoon[i]\n",
    "            Sch_pik_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i] + trip_L[i] + trip_Wafnoon[i] + trip_Safnoon[i]\n",
    "\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][0:Sch_drop_S], 'zone', z_home)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Sch_drop_S:Sch_drop_E], 'zone', z_school)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Sch_drop_E:Wrk_Mor_E], 'zone', z_work)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Wrk_Mor_E:Shp_E], 'zone', z_shop)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Shp_E:Wrk_Aft_E], 'zone', z_work)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Wrk_Aft_E:Sch_pik_E], 'zone', z_school)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Sch_pik_E:], 'zone', z_home)\n",
    "        demand_data['zone'].to_csv('ActivitySchedule_' + str(y) + '.csv', header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import simuated data\n",
    "# using function as the file can be closed automatically\n",
    "def rawimport(path, columnname):\n",
    "    agentdata = pd.read_csv(path, delimiter=' ', header=None, names=columnname)\n",
    "    agentdata = agentdata.drop(labels=\"todrop\", axis=1)\n",
    "    return agentdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run NL model; \n",
    "def run_NL(mob_call):\n",
    "    bridge.command(mob_call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_input(path_i, columnname_i):\n",
    "\n",
    "    agentdata = rawimport(path_i, columnname_i)\n",
    "    dt_stamp = [datetime.datetime.strptime(x,  '%Y-%m-%d %H:%M') for x in agentdata[\"acc_time\"]]\n",
    "    dow = [datetime.datetime.isoweekday(x) for x in dt_stamp]\n",
    "    hr = [x.hour for x in dt_stamp]\n",
    "    minu = [x.minute for x in dt_stamp]\n",
    "    agentdata['dow'] = dow\n",
    "    agentdata['hour'] = hr\n",
    "    agentdata['minute'] = minu\n",
    "\n",
    "    return (agentdata)\n",
    "\n",
    "#     mobiledata_ML = mobiledata.drop('radius', axis = 1)\n",
    "#     mobiledata_ML.drop('acc_time', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('acc_min', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('tid', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('txcor', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('tycor', axis=1, inplace=True)\n",
    "    \n",
    "#     agentdata.drop('acc_time', axis=1, inplace=True)\n",
    "#     agentdata.drop('acc_min', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "#    return (agentdata, mobiledata_ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_adj (dataset, tobedropped):\n",
    "    \n",
    "    for i in tobedropped:\n",
    "        dataset.drop(i, axis=1, inplace=True)\n",
    "    \n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReturnedValue(object):\n",
    "    def __init__(self, clf1, clf2, clf3, clf4):\n",
    "        self.clf1 = clf1\n",
    "        self.clf2 = clf2\n",
    "        self.clf3 = clf3\n",
    "        self.clf4 = clf4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NL_ML(mobiledata_ML, day_new, traintest_pre):\n",
    "    # prepare for ML\n",
    "    \n",
    "    # need to keep previous training/testing samples, otherwise results flactuates\n",
    "    \n",
    "    mobiledata_new = mobiledata_ML[mobiledata_ML['day']>=day_new]\n",
    "    \n",
    "    X_new = mobiledata_new.drop('tzone', axis=1)\n",
    "    X_new = X_new.drop('day', axis=1)\n",
    "    y_new = mobiledata_new['tzone']\n",
    "    #multiply by 10 to remove decimals\n",
    "    y_new = y_new*10\n",
    "    trainX_new, testX_new, trainY_new, testY_new = train_test_split(X_new, y_new, random_state = 10)\n",
    "        \n",
    "    trainX = pd.concat([traintest_pre[0], trainX_new])\n",
    "    testX = pd.concat([traintest_pre[1], testX_new])\n",
    "    trainY = pd.concat([traintest_pre[2], trainY_new])\n",
    "    testY = pd.concat([traintest_pre[3], testY_new])\n",
    "    \n",
    "    X = mobiledata_ML.drop('tzone', axis=1)\n",
    "    y = mobiledata_ML['tzone']\n",
    "    \n",
    "    # knn\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "    pred_knn = clf_knn.predict(testX)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    AC_knn = accuracy_score(testY, pred_knn)\n",
    "    RC_knn = recall_score(testY, pred_knn, average=None).mean()\n",
    "    CF_knn = mean_squared_error(testY, pred_knn)\n",
    "#    CV_knn = cross_val_score(clf_knn, X, y, cv=3)\n",
    "    ## using ridge regression\n",
    "    clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "    pred_rdg = clf_rdg.predict(testX)\n",
    "    AC_rdg = accuracy_score(testY, pred_rdg)\n",
    "    RC_rdg = recall_score(testY, pred_rdg, average=None).mean()  \n",
    "    CF_rdg =  mean_squared_error(testY, pred_rdg)\n",
    "#    CV_rdg = cross_val_score(clf_rdg, X, y, cv=3)\n",
    "    ## using RF\n",
    "    clf_rf = RandomForestClassifier(n_estimators=100, max_depth=100)\n",
    "    clf_rf.fit(trainX, trainY)\n",
    "    pred_rf = clf_rf.predict(testX)\n",
    "    AC_rf = accuracy_score(testY, pred_rf)\n",
    "    RC_rf = recall_score(testY, pred_rf, average=None).mean()   \n",
    "    CF_rf = mean_squared_error(testY, pred_rf)\n",
    "#    CV_rf = cross_val_score(clf_rf, X, y, cv=3)\n",
    "    ## using GBDT\n",
    "    clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "    clf_gb.fit(trainX, trainY)\n",
    "    pred_gb = clf_gb.predict(testX)\n",
    "    AC_gb = accuracy_score(testY, pred_gb)\n",
    "    RC_gb = recall_score(testY, pred_gb, average=None).mean()  \n",
    "    CF_gb = mean_squared_error(testY, pred_gb)\n",
    "#    CV_gb = cross_val_score(clf_gb, X, y, cv=3)\n",
    "\n",
    "    ##using neural networks\n",
    "    trainY_ = trainY.copy()\n",
    "    trainY_[trainY_==10] = 0\n",
    "    trainY_[trainY_==11] = 1\n",
    "    trainY_[trainY_==12] = 2\n",
    "    trainY_[trainY_==13] = 3\n",
    "    trainY_ = trainY_.astype(int)\n",
    "    \n",
    "    pred, optimal_params = fit_predict_NN(trainX.values, trainY_.values, testX.values, [T.tanh]*2, [10, 5], 0.5)\n",
    "    #roc_auc_score(testY_, pred)\n",
    "    pred_ = pred.copy()\n",
    "    pred_[pred==0] = 10\n",
    "    pred_[pred==1] = 11\n",
    "    pred_[pred==2] = 12\n",
    "    pred_[pred==3] = 13\n",
    "    AC_ANN = accuracy_score(testY, pred_)\n",
    "    RC_ANN = recall_score(testY, pred_, average=None).mean()\n",
    "    CF_ANN = mean_squared_error(testY, pred_)\n",
    "    \n",
    "    trained_clf = [clf_knn, clf_rdg, clf_rf, clf_gb, optimal_params]\n",
    "#    trained_CV = [CV_knn, CV_rdg, CV_rf, CV_gb]\n",
    "    trained_RC = [RC_knn, RC_rdg, RC_rf, RC_gb, RC_ANN]\n",
    "    trained_CM = [CF_knn, CF_rdg, CF_rf, CF_gb, CF_ANN]\n",
    "    trained_AC = [AC_knn, AC_rdg, AC_rf, AC_gb, AC_ANN]\n",
    "    traintest = [trainX, testX, trainY, testY]\n",
    "    \n",
    "    return (trained_clf, trained_RC, trained_CM, trained_AC, traintest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_agent(trained_clf, agentdata, last_n, traintest_prev):\n",
    "    last_minutes = last_n * (-1440)\n",
    "    pred_X = pd.DataFrame(None)\n",
    "    pred_X['pid'] = agentdata['pid'][last_minutes:]\n",
    "    pred_X['tpid'] = 0\n",
    "    pred_X = pd.concat([pred_X, agentdata[last_minutes:].iloc[:,-3:]], axis = 1)\n",
    "\n",
    "#    pred_X['day'] = pred_X['day'] + 1\n",
    "#    pred_X['acc_time'] = pred_X['acc_time'] + 1440\n",
    "    pred_y = agentdata[last_minutes:].iloc[:,2]\n",
    "    pred_y = pred_y*10\n",
    "\n",
    "    pred_knn = trained_clf[0].predict(pred_X)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    knn_AC = accuracy_score(pred_y, pred_knn)\n",
    "    knn_RC = recall_score(pred_y, pred_knn, average='macro')\n",
    "    knn_CM = mean_squared_error(pred_y, pred_knn)\n",
    "\n",
    "    ## using ridge regression\n",
    "    pred_rdg = trained_clf[1].predict(pred_X)\n",
    "    rdg_AC = accuracy_score(pred_y, pred_rdg)\n",
    "    rdg_RC = recall_score(pred_y, pred_rdg, average='macro')  \n",
    "    rdg_CM = mean_squared_error(pred_y, pred_rdg)\n",
    "\n",
    "    ## using RF\n",
    "    pred_rf = trained_clf[2].predict(pred_X)\n",
    "    rf_AC = accuracy_score(pred_y, pred_rf)\n",
    "    rf_RC = recall_score(pred_y, pred_rf, average='macro')  \n",
    "    rf_CM = mean_squared_error(pred_y, pred_rf)\n",
    "\n",
    "    ## using GBDT\n",
    "    pred_gb = trained_clf[3].predict(pred_X)\n",
    "    gb_AC = accuracy_score(pred_y, pred_gb)\n",
    "    gb_RC = recall_score(pred_y, pred_gb, average='macro')  \n",
    "    gb_CM = mean_squared_error(pred_y, pred_gb)\n",
    "    \n",
    "    ## using ANN\n",
    "    pred_ann = predict_NN(pred_X.values, traintest_prev[3], [T.tanh]*2, [10, 5], trained_clf[4])\n",
    "    pred_ann_ = pred_ann.copy()\n",
    "    pred_ann_[pred_ann==0] = 10\n",
    "    pred_ann_[pred_ann==1] = 11\n",
    "    pred_ann_[pred_ann==2] = 12\n",
    "    pred_ann_[pred_ann==3] = 13\n",
    "    AC_ANN = accuracy_score(pred_y, pred_ann_)\n",
    "    RC_ANN = recall_score(pred_y, pred_ann_, average='macro')\n",
    "    CF_ANN = mean_squared_error(pred_y, pred_ann_)\n",
    "    \n",
    "    pr_RC = [knn_RC, rdg_RC, rf_RC, gb_RC, RC_ANN]\n",
    "    pr_CM = [knn_CM, rdg_CM, rf_CM, gb_CM, CF_ANN]\n",
    "    pr_AC = [knn_AC, rdg_AC, rf_AC, gb_AC, AC_ANN]\n",
    "    pr_data = [pred_X, pred_y]\n",
    "\n",
    "    return (pr_RC, pr_CM, pr_AC, pr_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANN - prediction only\n",
    "def predict_NN(testX, train_y, activate_functions, hidden_layers, fittedpar):\n",
    "    dim = testX.shape[1]\n",
    "    dim_y = len(np.unique(train_y))\n",
    "    param = T.vector()\n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        #going through hidden layers\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        N = dim_previous * dim_y\n",
    "        v_ = parameter[n_previous:n_previous + N].reshape((dim_previous, dim_y))\n",
    "        output = h.dot(v_)\n",
    "        n_previous += N\n",
    "        return T.nnet.softmax(output), n_previous\n",
    "    \n",
    "    data = T.matrix()\n",
    "#    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    prediction = T.argmax(activation(data, param)[0], axis=1)\n",
    "    predict = theano.function([data, param], prediction)\n",
    "    return predict(testX, fittedpar)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# artificial neural networks\n",
    "def fit_predict_NN(trainX, trainY, testX, activate_functions, hidden_layers, reg_lambda):\n",
    "    X_ = theano.shared(trainX, name='X')\n",
    "    y_ = theano.shared(trainY, name='y')\n",
    "    param = T.vector()\n",
    "    dim = trainX.shape[1]\n",
    "    num_examples = trainX.shape[0]\n",
    "    dim_y = len(np.unique(trainY))\n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        #going through hidden layers\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        N = dim_previous * dim_y\n",
    "        v_ = parameter[n_previous:n_previous + N].reshape((dim_previous, dim_y))\n",
    "        output = h.dot(v_)\n",
    "        n_previous += N\n",
    "        return T.nnet.softmax(output), n_previous\n",
    "\n",
    "    y_hat, dim_W = activation(X_, param)\n",
    "    # optimize\n",
    "    #regularisation\n",
    "    loss_reg = 1./num_examples * reg_lambda/2 * T.sum(T.sqr(param)) \n",
    "    #cost\n",
    "    loss = T.nnet.categorical_crossentropy(y_hat, y_).mean() + loss_reg\n",
    "    loss_function = theano.function([param], loss)\n",
    "    loss_grad = theano.function([param], theano.grad(loss, param))\n",
    "    \n",
    "    result = minimize(loss_function, jac=loss_grad, x0=np.random.normal(size=activation(X_, param)[1]))\n",
    "    optimal_params = result['x']\n",
    "#    print result\n",
    "#    forward_prop = theano.function([param], y_hat)\n",
    "#    theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile='img/nn-theano-forward_prop.png', format='png')\n",
    "#    SVG(theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, return_image=True, format='svg'))\n",
    "\n",
    "    # predict data\n",
    "    data = T.matrix()\n",
    "#    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    prediction = T.argmax(activation(data, param)[0], axis=1)\n",
    "    predict = theano.function([data, param], prediction)\n",
    "    return (predict(testX, optimal_params), optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[array([ 1.]), array([ 1.1]), array([ 1.2]), array([ 1.3])]\n",
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BinomialDeviance requires 2 classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-bb08b96ea279>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         train_classifier, train_RC, train_CM, train_AC, traintestsplit = NL_ML(mobiledata_ML, currentday,\n\u001b[1;32m---> 72\u001b[1;33m                                                                                         traintestsplit)\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mrun_NL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmob_call\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-a21e8dd336aa>\u001b[0m in \u001b[0;36mNL_ML\u001b[1;34m(mobiledata_ML, day_new, traintest_pre)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m## using GBDT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mclf_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mclf_gb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mpred_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_gb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mAC_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_gb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m_check_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubsample\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_classes)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m             raise ValueError(\"{0:s} requires 2 classes.\".format(\n\u001b[1;32m--> 474\u001b[1;33m                 self.__class__.__name__))\n\u001b[0m\u001b[0;32m    475\u001b[0m         \u001b[1;31m# we only need to fit one tree for binary clf.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBinomialDeviance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: BinomialDeviance requires 2 classes."
     ]
    }
   ],
   "source": [
    "## run Netlogo and carryout ML\n",
    "\n",
    "agent_num = 1 #define the number of agents to be simulated\n",
    "zone_num = 4  # define the number of zones\n",
    "activities_num = 4 #define the total type of activities (at the moment home, work, shopping and school, these are used to set locations)\n",
    "\n",
    "counter = 0\n",
    "ini_day = 5 # define the number of days to initiate the model (i.e. the original training data)\n",
    "\n",
    "last_n = 1 # define the number of days to be left out from simulation data for testing purpose\n",
    "tot_day = 100 # define the number of days to be simulated\n",
    "sim_day = 1 # define for one simulation how many days to be simulated\n",
    "#num_sim = 1440 # define the duration of simulation (in minutes)\n",
    "CV_range = np.arange(0, 0.51, 0.02) #define the variance of schedule profiles\n",
    "\n",
    "# setting data storage\n",
    "Results_sum = np.zeros((len(CV_range), 2, 3, 5, tot_day)) # CV, TR/PR, RC/CM/AC, CLF, Days\n",
    "agentdata_tot = {}\n",
    "mobiledata_tot = {}\n",
    "smdata_tot = {}\n",
    "traintestsplit_tot = {}\n",
    "pr_data_tot = {}\n",
    "# agentdata_tot = np.zeros((len(CV_range), tot_day)) \n",
    "# mobiledata_tot = np.zeros((len(CV_range), tot_day)) \n",
    "# traintestsplit_tot = np.zeros((len(CV_range), tot_day))\n",
    "# pr_data_tot = np.zeros((len(CV_range), tot_day)) \n",
    "\n",
    "# generating demand profile\n",
    "for y in CV_range:\n",
    "    print y\n",
    "    population_gen(agent_num, zone_num, activities_num)\n",
    "    Schedule_Gen(agent_num, y, tot_day + ini_day + 1)\n",
    "\n",
    "    ## initiating the model, \n",
    "    model_init(ini_day*1440)\n",
    "    traintestsplit = [None, None, None, None]\n",
    "    \n",
    "    sim_min = sim_day*1440\n",
    "    mob_call = \"repeat \" + str(sim_min) + \" [go]\" \n",
    "    run_NL(mob_call)\n",
    "    \n",
    "    path = './agentdata.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "    agentdata  = data_input(path, columnname)\n",
    "    \n",
    "    path = './mobiletowers.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "    mobiledata_ML = data_input(path, columnname)\n",
    "    \n",
    "    path = './socialmedia.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"smpid\", \"smzone\", \"smxcor\", \"smycor\"]\n",
    "    smdata_ML = data_input(path, columnname)\n",
    "    \n",
    "    todrop = [\"acc_time\", \"acc_min\"]\n",
    "    agentdata = data_adj(agentdata, todrop)\n",
    "    \n",
    "    todrop = [\"radius\", \"acc_time\", \"acc_min\", \"tid\", \"txcor\", \"tycor\"]\n",
    "    mobiledata_ML = data_adj(mobiledata_ML, todrop)\n",
    "    \n",
    "    todrop = [\"acc_time\", \"acc_min\", \"smxcor\", \"smycor\"]\n",
    "    smdata_ML = data_adj(smdata_ML, todrop)\n",
    "    \n",
    "    agentdata_tot[counter, 0] = agentdata\n",
    "    mobiledata_tot[counter, 0] = mobiledata_ML\n",
    "    smdata_tot[counter, 0] = smdata_ML\n",
    "    ## loop + prediction\n",
    "    for x in range(ini_day, tot_day, sim_day):\n",
    "        print x\n",
    "        currentday = x\n",
    "        \n",
    "        train_classifier, train_RC, train_CM, train_AC, traintestsplit = NL_ML(mobiledata_ML, currentday,\n",
    "                                                                                        traintestsplit)\n",
    "        \n",
    "        run_NL(mob_call)\n",
    "        \n",
    "        path = './agentdata.txt'\n",
    "        columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "        agentdata  = data_input(path, columnname)\n",
    "\n",
    "        path = './mobiletowers.txt'\n",
    "        columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "        mobiledata_ML = data_input(path, columnname)\n",
    "\n",
    "        path = './socialmedia.txt'\n",
    "        columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"smpid\", \"smzone\", \"smxcor\", \"smycor\"]\n",
    "        smdata_ML = data_input(path, columnname)\n",
    "\n",
    "        todrop = [\"acc_time\", \"acc_min\"]\n",
    "        agentdata = data_adj(agentdata, todrop)\n",
    "\n",
    "        todrop = [\"radius\", \"acc_time\", \"acc_min\", \"tid\", \"txcor\", \"tycor\"]\n",
    "        mobiledata_ML = data_adj(mobiledata_ML, todrop)\n",
    "        \n",
    "        todrop = [\"acc_time\", \"acc_min\", \"smxcor\", \"smycor\"]\n",
    "        smdata_ML = data_adj(smdata_ML, todrop)\n",
    "\n",
    "#         agentdata_tot[counter, 0] = agentdata\n",
    "#         mobiledata_tot[counter, 0] = mobiledata_ML\n",
    "#         smdata_tot[counter, 0] = smdata_ML\n",
    "        \n",
    "        pred_RC, pred_CM, pred_AC, pred_data = pred_agent(train_classifier, agentdata, last_n, traintestsplit)\n",
    "        \n",
    "        \n",
    "        agentdata_tot[counter, x+1] = agentdata\n",
    "        mobiledata_tot[counter, x+1] = mobiledata_ML\n",
    "        smdata_tot[counter, x+1] = smdata_ML\n",
    "        traintestsplit_tot[counter, x] = traintestsplit\n",
    "        pr_data_tot[counter, x] = pred_data\n",
    "        \n",
    "        Results_sum[counter, 0, 0, 0, x] = train_RC[0] # CV, TR, RC, KNN, days\n",
    "        Results_sum[counter, 0, 0, 1, x] = train_RC[1] # CV, TR, RC, rdg, days\n",
    "        Results_sum[counter, 0, 0, 2, x] = train_RC[2] # CV, TR, RC, rf, days\n",
    "        Results_sum[counter, 0, 0, 3, x] = train_RC[3] # CV, TR, RC, gb, days\n",
    "        Results_sum[counter, 0, 0, 4, x] = train_RC[4] # CV, TR, RC, ann, days\n",
    "        \n",
    "        Results_sum[counter, 0, 1, 0, x] = train_AC[0] # CV, TR, AC, KNN, days\n",
    "        Results_sum[counter, 0, 1, 1, x] = train_AC[1] # CV, TR, AC, rdg, days\n",
    "        Results_sum[counter, 0, 1, 2, x] = train_AC[2] # CV, TR, AC, rf, days\n",
    "        Results_sum[counter, 0, 1, 3, x] = train_AC[3] # CV, TR, AC, gb, days\n",
    "        Results_sum[counter, 0, 1, 4, x] = train_AC[4] # CV, TR, AC, ann, days\n",
    "        \n",
    "        Results_sum[counter, 0, 2, 0, x] = train_CM[0] # CV, TR, CM, KNN, days\n",
    "        Results_sum[counter, 0, 2, 1, x] = train_CM[1] # CV, TR, CM, rdg, days\n",
    "        Results_sum[counter, 0, 2, 2, x] = train_CM[2] # CV, TR, CM, rf, days\n",
    "        Results_sum[counter, 0, 2, 3, x] = train_CM[3] # CV, TR, CM, gb, days\n",
    "        Results_sum[counter, 0, 2, 4, x] = train_CM[4] # CV, TR, CM, ann, days\n",
    "        \n",
    "        Results_sum[counter, 1, 0, 0, x] = pred_RC[0] # CV, PR, RC, KNN, days\n",
    "        Results_sum[counter, 1, 0, 1, x] = pred_RC[1] # CV, PR, RC, rdg, days\n",
    "        Results_sum[counter, 1, 0, 2, x] = pred_RC[2] # CV, PR, RC, rf, days\n",
    "        Results_sum[counter, 1, 0, 3, x] = pred_RC[3] # CV, PR, RC, gb, days\n",
    "        Results_sum[counter, 1, 0, 4, x] = pred_RC[4] # CV, PR, RC, ann, days\n",
    "        \n",
    "        Results_sum[counter, 1, 1, 0, x] = pred_AC[0] # CV, PR, AC, KNN, days\n",
    "        Results_sum[counter, 1, 1, 1, x] = pred_AC[1] # CV, PR, AC, rdg, days\n",
    "        Results_sum[counter, 1, 1, 2, x] = pred_AC[2] # CV, PR, AC, rf, days\n",
    "        Results_sum[counter, 1, 1, 3, x] = pred_AC[3] # CV, PR, AC, gb, days\n",
    "        Results_sum[counter, 1, 1, 4, x] = pred_AC[4] # CV, PR, AC, ann, days\n",
    "        \n",
    "        Results_sum[counter, 1, 2, 0, x] = pred_CM[0] # CV, PR, CM, KNN, days\n",
    "        Results_sum[counter, 1, 2, 1, x] = pred_CM[1] # CV, PR, CM, rdg, days\n",
    "        Results_sum[counter, 1, 2, 2, x] = pred_CM[2] # CV, PR, CM, rf, days\n",
    "        Results_sum[counter, 1, 2, 3, x] = pred_CM[3] # CV, PR, CM, gb, days\n",
    "        Results_sum[counter, 1, 2, 4, x] = pred_CM[4] # CV, PR, CM, ann, days\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>pid</th>\n",
       "      <th>tpid</th>\n",
       "      <th>tzone</th>\n",
       "      <th>dow</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day  pid  tpid  tzone  dow  hour  minute\n",
       "0     0  121     0    1.0    4     3      21\n",
       "1     0  121     0    1.0    4     5       2\n",
       "2     0  121     0    1.3    4     8      41\n",
       "3     0  121     0    1.1    4    10      49\n",
       "4     0  121     0    1.1    4    12      39\n",
       "5     0  121     0    1.1    4    14      30\n",
       "6     0  121     0    1.1    4    15      37\n",
       "7     0  121     0    1.1    4    16      23\n",
       "8     1  121     0    1.3    5     8      36\n",
       "9     1  121     0    1.1    5    11       0\n",
       "10    1  121     0    1.1    5    17      54\n",
       "11    1  121     0    1.0    5    19      41\n",
       "12    2  121     0    1.1    6     9      14\n",
       "13    2  121     0    1.1    6    11       7\n",
       "14    2  121     0    1.1    6    11      59\n",
       "15    2  121     0    1.0    6    20       9\n",
       "16    2  121     0    1.0    6    20      58\n",
       "17    3  121     0    1.0    7     0      45\n",
       "18    3  121     0    1.0    7     2      40\n",
       "19    3  121     0    1.0    7     3      19\n",
       "20    3  121     0    1.0    7     3      43\n",
       "21    3  121     0    1.0    7     7      11\n",
       "22    3  121     0    1.3    7     8      46\n",
       "23    3  121     0    1.1    7    12       5\n",
       "24    3  121     0    1.1    7    12      24\n",
       "25    3  121     0    1.1    7    17      28\n",
       "26    4  121     0    1.1    1    10      40\n",
       "27    4  121     0    1.1    1    11      50\n",
       "28    4  121     0    1.1    1    12      11\n",
       "29    4  121     0    1.1    1    12      29\n",
       "30    4  121     0    1.1    1    14      58\n",
       "31    4  121     0    1.0    1    19      26\n",
       "32    4  121     0    1.0    1    19      36\n",
       "33    4  121     0    1.0    1    19      55\n",
       "34    5  121     0    1.1    2    10      53\n",
       "35    5  121     0    1.1    2    11      11\n",
       "36    5  121     0    1.1    2    14      46\n",
       "37    5  121     0    1.1    2    16      55\n",
       "38    5  121     0    1.1    2    17      26"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobiledata_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainY_ = trainY.copy()\n",
    "trainY_[trainY_==15] = 0\n",
    "trainY_[trainY_==25] = 1\n",
    "trainY_[trainY_==35] = 2\n",
    "trainY_[trainY_==45] = 3\n",
    "trainY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = trainX[:, 0].min() - .5, trainX[:, 0].max() + .5\n",
    "    y_min, y_max = trainX[:, 1].min() - .5, trainX[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(trainX[:, 0], trainX[:, 1], c=trainY, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.gca(projection='3d')\n",
    "X = range(0,26)\n",
    "Y = range(0,100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = Results_sum[X, 1, 0, 3, Y]\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "vals = ax.get_xticks()\n",
    "ax.set_xticklabels(CV_range[[x for x in vals]])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Results_sum[0, 1, 2, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2]\n",
    "ax.plot(Results_sum[0, 1, 1, 0, :], label='KNN', color=\"g\")\n",
    "ax.plot(Results_sum[0, 1, 1, 1, :], label='Ridge', color=\"b\")\n",
    "ax.plot(Results_sum[0, 1, 1, 2, :], label='RF', color=\"y\")\n",
    "ax.plot(Results_sum[0, 1, 1, 3, :], label='GBDT', color=\"r\")\n",
    "ax.plot(Results_sum[0, 1, 1, 4, :], label='ANN', color=\"k\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of days\")\n",
    "ax.set_xticks(range(0, len(Results_sum[0, 1, 1, 0, :]), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')\n",
    "plt.title('Accuracy on the new predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(Results_sum[0, 0, 1, 0, :], label='KNN', color=\"g\")\n",
    "ax.plot(Results_sum[0, 0, 1, 1, :], label='Ridge', color=\"b\")\n",
    "ax.plot(Results_sum[0, 0, 1, 2, :], label='RF', color=\"y\")\n",
    "ax.plot(Results_sum[0, 0, 1, 3, :], label='GBDT', color=\"r\")\n",
    "ax.plot(Results_sum[0, 0, 1, 4, :], label='ANN', color=\"k\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of simulated days\")\n",
    "ax.set_xticks(range(0, len(Results_sum[0, 0, 1, 0, :]), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')\n",
    "plt.title('Accuracy on the test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(Results_sum[0, 1, 0, 0, :], label='KNN', color=\"g\")\n",
    "ax.plot(Results_sum[0, 1, 0, 1, :], label='Ridge', color=\"b\")\n",
    "ax.plot(Results_sum[0, 1, 0, 2, :], label='RF', color=\"y\")\n",
    "ax.plot(Results_sum[0, 1, 0, 3, :], label='GBDT', color=\"r\")\n",
    "ax.plot(Results_sum[0, 1, 0, 4, :], label='ANN', color=\"k\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of simulated days\")\n",
    "ax.set_xticks(range(0, len(Results_sum[0, 1, 0, 0, :]), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')\n",
    "plt.title('recall on the new predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(Results_sum[0, 1, 1, 0, :], label='KNN', color=\"g\")\n",
    "ax.plot(Results_sum[0, 1, 1, 1, :], label='Ridge', color=\"b\")\n",
    "ax.plot(Results_sum[0, 1, 1, 2, :], label='RF', color=\"y\")\n",
    "ax.plot(Results_sum[0, 1, 1, 3, :], label='GBDT', color=\"r\")\n",
    "ax.plot(Results_sum[0, 1, 1, 4, :], label='ANN', color=\"k\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of simulated days\")\n",
    "ax.set_xticks(range(0, len(Results_sum[0, 1, 1, 0, :]), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')\n",
    "plt.title('Recall on the test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobiledata_PL = mobiledata_tot[0,99].copy()\n",
    "mobiledata_PL['min_acc'] = mobiledata_PL['hour'] * 60 + mobiledata_PL['minute']\n",
    "fig1 = plt.figure(figsize=(10,5))\n",
    "ax = fig1.add_subplot(1, 1, 1)\n",
    "ax.scatter(mobiledata_PL['min_acc'], mobiledata_PL['tzone'], marker = \"|\")\n",
    "ax.set_xticks(range(0, 1441, 1440/12))\n",
    "ax.set_yticks(range(0, 46, 15))\n",
    "vals = ax.get_xticks()\n",
    "print vals\n",
    "ax.set_xticklabels(['{:02.0f}h'.format(x/60) for x in vals])\n",
    "ax.set_ylabel(\"Zone number\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "# ax2.plot(mobiledata_sel['minute'], mobiledata_sel['tzone_y'])\n",
    "# ax2.set_xticks(range(0, 1440, 200))\n",
    "# ax2.set_yticks(range(0, 45, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checking the distribution of mobile phone calls\n",
    "fig2 = plt.figure(figsize=(10,5))\n",
    "ax = fig2.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "(mobiledata_PL['min_acc'][pd.notnull(mobiledata_PL['tzone'])]).hist(bins=12)\n",
    "ax.set_xticks(range(0, 1441, 1440/12))\n",
    "vals = ax.get_xticks()\n",
    "print vals\n",
    "ax.set_xticklabels(['{:02.0f}h'.format(x/60) for x in vals])\n",
    "ax.set_ylabel(\"Number of calls (accumulated)\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "#ax.set_xticks(range(0, 1440, 1440/12))\n",
    "#plt.hist(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobiledata_PL = mobiledata_ML.copy()\n",
    "mobiledata_PL['min_acc'] = mobiledata_PL['hour'] * 60 + mobiledata_PL['minute']\n",
    "print mobiledata_PL.max()\n",
    "print mobiledata_PL.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "A = np.random.rand(1000,10000).astype(theano.config.floatX)\n",
    "B = np.random.rand(10000,1000).astype(theano.config.floatX)\n",
    "np_start = time.time()\n",
    "AB = A.dot(B)\n",
    "np_end = time.time()\n",
    "X,Y = theano.tensor.matrices('XY')\n",
    "mf = theano.function([X,Y],X.dot(Y))\n",
    "t_start = time.time()\n",
    "tAB = mf(A,B)\n",
    "t_end = time.time()\n",
    "print(\"NP time: %f[s], theano time: %f[s] (times should be close when run on CPU!)\" %(\n",
    "                                           np_end-np_start, t_end-t_start))\n",
    "print(\"Result difference: %f\" % (np.abs(AB-tAB).max(), ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print aa[aa['minute']==789], bb[bb['minute']==789]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class User(object):\n",
    "    def __init__(self, name, email):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "    def commit(self):\n",
    "        pass\n",
    "\n",
    "jason = User('jason', 'jason@email.com')\n",
    "jack = User('jack', 'jack@yahoo.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    user_paths = os.environ['PYTHONPATH'].split(os.pathsep)\n",
    "except KeyError:\n",
    "    user_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = traintestsplit_tot[99]\n",
    "pred_X, pred_y = pr_data_tot[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainX.drop('acc_time', axis=1, inplace=True)\n",
    "trainX.drop('day', axis=1, inplace=True)\n",
    "#testX.drop('acc_time', axis=1, inplace=True)\n",
    "testX.drop('day', axis=1, inplace=True)\n",
    "#pred_X.drop('acc_time', axis=1, inplace=True)\n",
    "pred_X.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## using RF\n",
    "clf_rf = RandomForestClassifier()#n_estimators=1000, max_depth=100)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "print precision_score(testY, pred_rf, average=None)\n",
    "print accuracy_score(testY, pred_rf)\n",
    "print recall_score(testY, pred_rf, average='macro')\n",
    "print recall_score(testY, pred_rf, average='micro')\n",
    "print confusion_matrix(testY, pred_rf)\n",
    "\n",
    "pred_rf = clf_rf.predict(pred_X)\n",
    "print precision_score(pred_y, pred_rf, average=None)\n",
    "print accuracy_score(pred_y, pred_rf)\n",
    "print recall_score(pred_y, pred_rf, average='macro')\n",
    "print recall_score(pred_y, pred_rf, average='micro')\n",
    "print confusion_matrix(pred_y, pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average=None)\n",
    "print accuracy_score(testY, pred_knn)\n",
    "print recall_score(testY, pred_knn, average='macro')\n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "print confusion_matrix(testY, pred_knn)\n",
    "\n",
    "pred_knn = clf_knn.predict(pred_X)\n",
    "print precision_score(pred_y, pred_knn, average=None)\n",
    "print accuracy_score(pred_y, pred_knn)\n",
    "print recall_score(pred_y, pred_knn, average='macro')\n",
    "print recall_score(pred_y, pred_knn, average='micro')\n",
    "print confusion_matrix(pred_y, pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average=None)\n",
    "print accuracy_score(testY, pred_knn)\n",
    "print recall_score(testY, pred_knn, average='macro')\n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "print confusion_matrix(testY, pred_knn)\n",
    "\n",
    "pred_knn = clf_knn.predict(pred_X)\n",
    "print precision_score(pred_y, pred_knn, average=None)\n",
    "print accuracy_score(pred_y, pred_knn)\n",
    "print recall_score(pred_y, pred_knn, average='macro')\n",
    "print recall_score(pred_y, pred_knn, average='micro')\n",
    "print confusion_matrix(pred_y, pred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './agentdata.txt'\n",
    "columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "agentdata = rawimport(path, columnname)\n",
    "dt_stamp = [datetime.strptime(x,  '%Y-%m-%d %H:%M') for x in agentdata[\"acc_time\"]]\n",
    "dow = [datetime.isoweekday(x) for x in dt_stamp]\n",
    "hr = [x.hour for x in dt_stamp]\n",
    "minu = [x.minute for x in dt_stamp]\n",
    "agentdata['dow'] = dow\n",
    "agentdata['hour'] = hr\n",
    "agentdata['minute'] = minu\n",
    "\n",
    "\n",
    "path = './mobiletowers.txt'\n",
    "columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "mobiledata = rawimport(path, columnname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_stamp = []\n",
    "for x in agentdata[\"acc_time\"].values:\n",
    "    dt_stamp.append(datetime.strptime(x, '%Y-%m-%d %H:%M'))\n",
    "dt_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'agentdata['hour'] = hr\n",
    "agentdata['minute'] = minu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for x in agentdata[\"acc_time\"].values:\n",
    "    print parse(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leave the last n day(s) out for testing purpose\n",
    "\n",
    "#mobiledata_ML = mobiledata[mobiledata['day'] <= (mobiledata['day'].max() - last_n)]\n",
    "mobiledata_ML = mobiledata.drop('radius', axis = 1)\n",
    "mobiledata_ML.drop('tid', axis=1, inplace=True)\n",
    "mobiledata_ML.drop('txcor', axis=1, inplace=True)\n",
    "mobiledata_ML.drop('tycor', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare for ML\n",
    "X = mobiledata_ML.drop('tzone', axis=1)\n",
    "y = mobiledata_ML['tzone']\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k fold cross-validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "for train_indices, test_indices in KFold(len(X), n_folds=3):\n",
    "    fold_trainX = X.iloc[train_indices, :]\n",
    "    fold_testX  = X.iloc[test_indices, :]\n",
    "    fold_trainY = y[train_indices]\n",
    "    fold_testY  = y[test_indices]\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=2).fit(fold_trainX, fold_trainY)\n",
    "    pred_knn = clf_knn.predict(fold_testX)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    print precision_score(fold_testY, pred_knn, average='micro')  \n",
    "    print recall_score(fold_testY, pred_knn, average='micro')\n",
    "    confusion_matrix(fold_testY, pred_knn)\n",
    "    #print fold_trainY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# knn\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average=None)\n",
    "print accuracy_score(testY, pred_knn)\n",
    "print recall_score(testY, pred_knn, average='macro')\n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "print confusion_matrix(testY, pred_knn)\n",
    "cross_val_score(clf_knn, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using ridge regression\n",
    "clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "pred_rdg = clf_rdg.predict(testX)\n",
    "print precision_score(testY, pred_rdg, average='micro')  \n",
    "print confusion_matrix(testY, pred_rdg)\n",
    "cross_val_score(clf_rdg, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using RF\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=100)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "print precision_score(testY, pred_rf, average='micro')  \n",
    "print confusion_matrix(testY, pred_rf)\n",
    "cross_val_score(clf_rf, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using GBDT\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "clf_gb.fit(trainX, trainY)\n",
    "pred_gb = clf_gb.predict(testX)\n",
    "print precision_score(testY, pred_gb, average='micro')  \n",
    "print confusion_matrix(testY, pred_gb)\n",
    "cross_val_score(clf_gb, X, y, cv=3)\n",
    "#test_qualities = []\n",
    "# for p in clf_gb.staged_predict_proba(testX):\n",
    "#     test_qualities.append(precision_score(testY, p[:, 1]))\n",
    "# plt.plot(test_qualities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_n = 1\n",
    "last_minutes = last_n * (-1440)\n",
    "pred_X = agentdata[last_minutes:].iloc[:,0:4]\n",
    "pred_X['tpid'] = 0\n",
    "pred_y = agentdata[last_minutes:].iloc[:,4]\n",
    "\n",
    "# pred_knn = trained_clf.clf1.predict(pred_X)\n",
    "# #print roc_auc_score(testY, pred_knn)\n",
    "# knn_AC = accuracy_score(pred_y, pred_knn)\n",
    "# knn_RC = recall_score(pred_y, pred_knn, average='macro')\n",
    "# knn_CM = confusion_matrix(pred_y, pred_knn)\n",
    "\n",
    "# ## using ridge regression\n",
    "# pred_rdg = trained_clf.clf2.predict(pred_X)\n",
    "# rdg_AC = accuracy_score(pred_y, pred_rdg)\n",
    "# rdg_RC = recall_score(pred_y, pred_rdg, average='macro')  \n",
    "# rdg_CM = confusion_matrix(pred_y, pred_rdg)\n",
    "\n",
    "# ## using RF\n",
    "# pred_rf = trained_clf.clf3.predict(pred_X)\n",
    "# rf_AC = accuracy_score(pred_y, pred_rf)\n",
    "# rf_RC = recall_score(pred_y, pred_rf, average='macro')  \n",
    "# rf_CM = confusion_matrix(pred_y, pred_rf)\n",
    "\n",
    "## using GBDT\n",
    "pred_gb = clf_gb.predict(pred_X)\n",
    "gb_AC = accuracy_score(pred_y, pred_gb)\n",
    "gb_RC = recall_score(pred_y, pred_gb, average='macro')  \n",
    "gb_CM = confusion_matrix(pred_y, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using SVM\n",
    "## solving multi-classification problem?\n",
    "clf_svc = SVC()\n",
    "clf_svc.fit(trainX, trainY)\n",
    "pred_svc = clf_svc.predict(testX)\n",
    "print precision_score(testY, pred_svc, average='micro')  \n",
    "print confusion_matrix(testY, pred_svc)\n",
    "cross_val_score(clf_svc, X, y, cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## prediction\n",
    "# pre_agent1 = mobiledata[:1]\n",
    "# pre_agent = pd.concat([pre_agent1]*1440)\n",
    "# pre_agent['day']=mobiledata['day'].max()+1\n",
    "# pre_agent['minute']=range(0,1440)\n",
    "# pre_agent['acc_time']=pre_agent['minute'] + pre_agent['day']*1440\n",
    "# pre_agent.drop('tzone', axis=1, inplace=True)\n",
    "# pre_agent.drop('radius', axis=1, inplace=True)\n",
    "# pre_agent.drop('tid', axis=1, inplace=True)\n",
    "# pre_agent.drop('txcor', axis=1, inplace=True)\n",
    "# pre_agent.drop('tycor', axis=1, inplace=True)\n",
    "pred_X = agentdata[-1440:].iloc[:,0:4]\n",
    "pred_X['tpid'] = 0\n",
    "pred_y = agentdata[-1440:].iloc[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_knn = clf_knn.predict(pred_X)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "knn_PR = precision_score(pred_y, pred_knn, average='micro')\n",
    "knn_CM = confusion_matrix(pred_y, pred_knn)\n",
    "\n",
    "## using ridge regression\n",
    "pred_rdg = clf_rdg.predict(pred_X)\n",
    "rdg_PR = precision_score(pred_y, pred_rdg, average='micro')  \n",
    "rdg_CM = confusion_matrix(pred_y, pred_rdg)\n",
    "\n",
    "## using RF\n",
    "pred_rf = clf_rf.predict(pred_X)\n",
    "rf_PR = precision_score(pred_y, pred_rf, average='micro')  \n",
    "rf_CM = confusion_matrix(pred_y, pred_rf)\n",
    "\n",
    "## using GBDT\n",
    "pred_gb = clf_gb.predict(pred_X)\n",
    "gb_PR = precision_score(pred_y, pred_gb, average='micro')  \n",
    "gb_CM = confusion_matrix(pred_y, pred_gb)\n",
    "\n",
    "print knn_PR, rdg_PR, rf_PR, gb_PR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_gb\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare for ML\n",
    "X = mobiledata_fs_filled.drop('zone', axis=1)\n",
    "y = mobiledata_fs_filled['zone']\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)\n",
    "\n",
    "## using KNN\n",
    "# weights might need to be changed - more weights towards the recent data?\n",
    "# metrics? canberra, minkowski\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "knn_PR = precision_score(testY, pred_knn, average='micro')\n",
    "knn_CM = confusion_matrix(testY, pred_knn)\n",
    "\n",
    "## using ridge regression\n",
    "clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "pred_rdg = clf_rdg.predict(testX)\n",
    "rdg_PR = precision_score(testY, pred_rdg, average='micro')  \n",
    "rdg_CM = confusion_matrix(testY, pred_rdg)\n",
    "\n",
    "## using RF\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "rf_PR = precision_score(testY, pred_rf, average='micro')  \n",
    "rf_CM = confusion_matrix(testY, pred_rf)\n",
    "\n",
    "## using GBDT\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "clf_gb.fit(trainX, trainY)\n",
    "pred_gb = clf_gb.predict(testX)\n",
    "gb_PR = precision_score(testY, pred_gb, average='micro')  \n",
    "gb_CM = confusion_matrix(testY, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class ReturnValue(object):\n",
    "#    def __init__(self, y0, y1):\n",
    "#        self.y0 = y0\n",
    "#        self.y1 = y1\n",
    "\n",
    "def data_input():\n",
    "    path = './agentdata.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "    agentdata = rawimport(path, columnname)\n",
    "    \n",
    "    path = './mobiletowers.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "    mobiledata = rawimport(path, columnname)\n",
    "    \n",
    "    # extend data to full (missing data shown as NaN)\n",
    "    mobiledata = mobiledata.drop(labels = [\"tid\"], axis=1)\n",
    "\n",
    "    mobiledata_fs = pd.merge(agentdata, mobiledata, how = 'left', on = [\"acc_time\", 'day', 'minute', \"pid\"])\n",
    "    #mobiledata_fs = pd.merge(agentdata_ri, mobiledata_ri, how = 'left', left_index = True, right_index = True)\n",
    "    #print mobiledata_fs.count()\n",
    "    #mobiledata_fs\n",
    "# fill the missing data using NN (k=1)\n",
    "# the reason why \"day\" and \"minute\" need to be removed before imputation is that the imputation is done based on features.\n",
    "# in other words, the day and minute will have impacts on the imputed results\n",
    "# to maintain a continuity of a user between days it is necessary to remove \"day\" and \"minute\" before imputation and add them back later\n",
    "# also no need to imputate tower id\n",
    "    mobiledata_fs_test = mobiledata_fs.drop(labels = [\"day\"], axis=1)\n",
    "    mobiledata_fs_test = mobiledata_fs_test.drop(labels = [\"minute\"], axis=1)\n",
    "    mobiledata_fs_test = mobiledata_fs_test.drop(labels = [\"tid\"], axis=1)\n",
    "    mobiledata_fs_filled = KNN(k=1).complete(mobiledata_fs_test)\n",
    "    mobiledata_fs_filled = pd.DataFrame(mobiledata_fs_filled, columns = [\"acc_time\", \"pid\", \"zone\", \n",
    "                                                                     \"status\", \"xcor\", \"ycor\", \"tpid\", \"tzone\", \n",
    "                                                                     \"radius\", \"txcor\", \"tycor\"])\n",
    "# add the day and minute information back\n",
    "    mobiledata_daymin = mobiledata_fs.ix[:,[0, 1, 2, 3, 8]]\n",
    "    mobiledata_fs_filled = pd.merge(mobiledata_daymin, mobiledata_fs_filled, how = 'left', on = [\"acc_time\", \"pid\"])\n",
    "    mobiledata_cp = pd.merge(mobiledata_fs, mobiledata_fs_filled, how = 'left', on = [\"acc_time\", 'day', 'minute', \"pid\"])\n",
    "    mobiledata_fs_filled.fillna(-1, inplace=True)\n",
    "    \n",
    "    return (mobiledata_fs, mobiledata_fs_filled, mobiledata_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run_NL()\n",
    "\n",
    "mobiledata_fs, mobiledata_fs_filled, mobiledata_cp = data_input()\n",
    "#miss_imput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print mobiledata_fs_filled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobiledata_fs_filled.zone.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "mobiledata_sel = mobiledata_cp[mobiledata_cp['day'] == 0]\n",
    "ax1.scatter(mobiledata_sel['minute'], mobiledata_sel['tzone_x'])\n",
    "ax1.set_xticks(range(0, 1440, 200))\n",
    "ax1.set_yticks(range(0, 45, 15))\n",
    "ax2.plot(mobiledata_sel['minute'], mobiledata_sel['tzone_y'])\n",
    "ax2.set_xticks(range(0, 1440, 200))\n",
    "ax2.set_yticks(range(0, 45, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checking the distribution of mobile phone calls\n",
    "fig2 = plt.figure(figsize=(10,5))\n",
    "ax = fig2.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])]).hist(bins=12)\n",
    "#plt.hist(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ML\n",
    "def NL_ML(mobiledata_fs_filled):\n",
    "    # prepare for ML\n",
    "    X = mobiledata_fs_filled.drop('zone', axis=1)\n",
    "    y = mobiledata_fs_filled['zone']\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y)\n",
    "    \n",
    "    ## using KNN\n",
    "    # weights might need to be changed - more weights towards the recent data?\n",
    "    # metrics? canberra, minkowski\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "    pred_knn = clf_knn.predict(testX)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    knn_PR = precision_score(testY, pred_knn, average='micro')\n",
    "    knn_CM = confusion_matrix(testY, pred_knn)\n",
    "    \n",
    "    ## using ridge regression\n",
    "    clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "    pred_rdg = clf_rdg.predict(testX)\n",
    "    rdg_PR = precision_score(testY, pred_rdg, average='micro')  \n",
    "    rdg_CM = confusion_matrix(testY, pred_rdg)\n",
    "    \n",
    "    ## using RF\n",
    "    clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "    clf_rf.fit(trainX, trainY)\n",
    "    pred_rf = clf_rf.predict(testX)\n",
    "    rf_PR = precision_score(testY, pred_rf, average='micro')  \n",
    "    rf_CM = confusion_matrix(testY, pred_rf)\n",
    "    \n",
    "    ## using GBDT\n",
    "    clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "    clf_gb.fit(trainX, trainY)\n",
    "    pred_gb = clf_gb.predict(testX)\n",
    "    gb_PR = precision_score(testY, pred_gb, average='micro')  \n",
    "    gb_CM = confusion_matrix(testY, pred_gb)\n",
    "    \n",
    "    return (knn_PR, knn_CM, rdg_PR, rdg_CM, rf_PR, rf_CM, gb_PR, gb_CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(knn_all_PR, label='NN', color=\"g\")\n",
    "ax.plot(rdg_all_PR, label='Ridge', color=\"b\")\n",
    "ax.plot(rf_all_PR, label='RF', color=\"y\")\n",
    "ax.plot(gb_all_PR, label='GBDT', color=\"r\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax.set_ylabel(\"precision\")\n",
    "ax.set_xlabel(\"Call Frequency - number of calls per day\")\n",
    "ax.set_xticklabels(range(1,10))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_all_PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = NL_ML(comdata_fs_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gb_all_PR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare for ML\n",
    "\n",
    "X = mobiledata_fs_filled.drop('zone', axis=1)\n",
    "y = mobiledata_fs_filled['zone']\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using KNN\n",
    "# weights might need to be changed - more weights towards the recent data?\n",
    "# metrics? canberra, minkowski\n",
    "\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average='micro')  \n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "confusion_matrix(testY, pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_knn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testX, pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using Ridge\n",
    "\n",
    "clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "pred_rdg = clf_rdg.predict(testX)\n",
    "print precision_score(testY, pred_rdg, average='micro')  \n",
    "print recall_score(testY, pred_rdg, average='micro')\n",
    "confusion_matrix(testY, pred_rdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## using Lasso\n",
    "\n",
    "clf_las = Lasso(alpha=1).fit(trainX,trainY)\n",
    "pred_las = clf_las.predict(testX)\n",
    "print precision_score(testY, pred_las, average='micro')  \n",
    "print recall_score(testY, pred_las, average='micro')\n",
    "confusion_matrix(testY, pred_las)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using RF\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "print precision_score(testY, pred_rf, average='micro')  \n",
    "print recall_score(testY, pred_rf, average='micro')\n",
    "confusion_matrix(testY, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using GBDT\n",
    "\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "clf_gb.fit(trainX, trainY)\n",
    "pred_gb = clf_gb.predict(testX)\n",
    "print precision_score(testY, pred_gb, average='micro')  \n",
    "print recall_score(testY, pred_gb, average='micro')\n",
    "confusion_matrix(testY, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict_NNa(trainX, trainY, testX, activate_functions, hidden_layers, reg_lambda):\n",
    "    X_ = theano.shared(trainX, name='X')\n",
    "    y_ = theano.shared(trainY, name='y')\n",
    "    param = T.vector()\n",
    "    dim = X.shape[1]\n",
    "    num_examples = X.shape[0]\n",
    "    dim_y = len(np.unique(trainY))\n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        #going through hidden layers\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        N = dim_previous * dim_y\n",
    "        v_ = parameter[n_previous:n_previous + N].reshape((dim_previous, dim_y))\n",
    "        output = h.dot(v_)\n",
    "        n_previous += N\n",
    "        return T.nnet.softmax(output), n_previous\n",
    "\n",
    "    y_hat, dim_W = activation(X_, param)\n",
    "    # optimize\n",
    "    #regularisation\n",
    "    loss_reg = 1./num_examples * reg_lambda/2 * T.sum(T.sqr(param)) \n",
    "    #cost\n",
    "    loss = T.nnet.categorical_crossentropy(y_hat, y_).mean() + loss_reg\n",
    "    loss_function = theano.function([param], loss)\n",
    "    loss_grad = theano.function([param], theano.grad(loss, param))\n",
    "    \n",
    "    result = minimize(loss_function, jac=loss_grad, x0=np.random.normal(size=activation(X_, param)[1]))\n",
    "    optimal_params = result['x']\n",
    "#    print result\n",
    "    forward_prop = theano.function([param], y_hat)\n",
    "    theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile='img/nn-theano-forward_prop.png', format='png')\n",
    "    SVG(theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, return_image=True, format='svg'))\n",
    "\n",
    "    # predict data\n",
    "    data = T.matrix()\n",
    "#    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    prediction = T.argmax(activation(data, param)[0], axis=1)\n",
    "    predict = theano.function([data, param], prediction)\n",
    "    return predict(testX, optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define activation function\n",
    "#sigmoid (which we used, T.nnet.sigmoid)\n",
    "#leaky ReLU (defined below)\n",
    "#softplus (T.nnet.softplus)\n",
    "\n",
    "def LeakyReLU(x):\n",
    "    return T.switch(x > 0, x, 0.5 * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from IPython.display import Image\n",
    "X, y = make_moons(n_samples=20000, noise=0.1)\n",
    "trainX, testX, trainY, testY = train_test_split(X, y, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = traintestsplit_tot[0, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.vstack((trainX.values, testX.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_ = T.matrix('X') # matrix of doubles\n",
    "# y_ = T.lvector('y') # vector of int64\n",
    "\n",
    "pred = fit_predict_NN(trainX.values, trainY_.values, testX.values, [T.tanh]*2, [10, 5], 0.5)\n",
    "#roc_auc_score(testY_, pred)\n",
    "print precision_score(testY_, pred, average='micro')\n",
    "print recall_score(testY_, pred, average='micro')\n",
    "pred_ = pred.copy()\n",
    "pred_[pred==0] = 15\n",
    "pred_[pred==1] = 25\n",
    "pred_[pred==2] = 35\n",
    "pred_[pred==3] = 45\n",
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "#train_X, train_y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "X, y = sklearn.datasets.make_blobs(200, n_features=2, centers=3)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "# train_X = train_X.astype(np.float32)\n",
    "# train_y = train_y.astype(np.int32)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fit_predict_NN(train_X, train_y, test_X, [T.tanh], [100])\n",
    "print precision_score(test_y, pred, average='micro')\n",
    "print recall_score(test_y, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "aa = trainY.as_matrix()\n",
    "bb = aa.reshape((aa.shape[0], 1))\n",
    "trainY_ = MultiLabelBinarizer().fit_transform(bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainY_ = np.tile(np.zeros((1,4)), (trainY.shape[0],1))\n",
    "trainY_[trainY == 15] = [[1, 0, 0, 0]]\n",
    "# trainY_[trainY == 25] = [0, 1, 0, 0]\n",
    "# trainY_[trainY == 35] = [0, 0, 1, 0]\n",
    "# trainY_[trainY == 45] = [0, 0, 0, 1]\n",
    "trainY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "A = np.random.rand(1000,10000).astype(theano.config.floatX)\n",
    "B = np.random.rand(10000,1000).astype(theano.config.floatX)\n",
    "np_start = time.time()\n",
    "AB = A.dot(B)\n",
    "np_end = time.time()\n",
    "X,Y = theano.tensor.matrices('XY')\n",
    "mf = theano.function([X,Y],X.dot(Y))\n",
    "t_start = time.time()\n",
    "tAB = mf(A,B)\n",
    "t_end = time.time()\n",
    "print(\"NP time: %f[s], theano time: %f[s] (times should be close when run on CPU!)\" %(\n",
    "                                           np_end-np_start, t_end-t_start))\n",
    "print(\"Result difference: %f\" % (np.abs(AB-tAB).max(), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
