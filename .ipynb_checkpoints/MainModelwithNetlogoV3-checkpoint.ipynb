{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import functions\n",
    "# version 2: did not use imputation. just ML directly \n",
    "\n",
    "# parameters:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from IPython.display import Image\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import datetime\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from dateutil.parser import parse\n",
    "#from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute\n",
    "from sklearn.preprocessing import StandardScaler                 #normalising features\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "#from sknn import mlp\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from py4j.java_gateway import JavaGateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gw = JavaGateway() # New gateway connection\n",
    "bridge = gw.entry_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load netlogo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the model:\n",
    "models_path = \"D:/Work/PhD/Github/NetLogoModel/\"\n",
    "model_name = \"PhD_InitialModelV3.nlogo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelLocation = models_path + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/Work/PhD/Github/NetLogoModel/PhD_InitialModelV3.nlogo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bridge.openModel(modelLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run one iteration of the model, one command at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bridge.command(\"set grid-size 21\")\n",
    "bridge.command(\"set nb-people 1\")\n",
    "bridge.command(\"set Show_Names_Nodes? False\")\n",
    "bridge.command(\"set Show_Names_people? False\")\n",
    "bridge.command(\"set car-speed 1\")\n",
    "bridge.command(\"set mobiletowerdata? True\")\n",
    "bridge.command(\"set socialmedia_data? True\")\n",
    "bridge.command(\"set grids_covered_vector 10\")\n",
    "bridge.command(\"set avg_num_calls_perday 100\")\n",
    "bridge.command(\"set avg_call_duration_mins 2\")\n",
    "\n",
    "bridge.command(\"random-seed 0\")\n",
    "bridge.command(\"setup\")\n",
    "bridge.command(\"repeat 1440 [go]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bridge.command(\"repeat 144000 [go]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Have the model report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 15 ./agentdata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 15 ./mobiletowers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiating the netlogo model\n",
    "# and run it for the first 10 days\n",
    "def model_init(ini_mins):\n",
    "    bridge.command(\"set grid-size 21\")\n",
    "    bridge.command(\"set nb-people 1\")\n",
    "    bridge.command(\"set Show_Names_Nodes? False\")\n",
    "    bridge.command(\"set Show_Names_people? False\")\n",
    "    bridge.command(\"set car-speed 1\")\n",
    "    bridge.command(\"set mobiletowerdata? True\")\n",
    "    bridge.command(\"set socialmedia_data? True\")\n",
    "    bridge.command(\"set grids_covered_vector 5\")\n",
    "    bridge.command(\"set avg_num_calls_perday 10\")            #setting for the usage of mobile phones\n",
    "    bridge.command(\"set avg_call_duration_mins 2\")\n",
    "    bridge.command(\"set avg_num_sm_usage 5\")                 #setting for the usage of social media\n",
    "    bridge.command(\"random-seed 0\")\n",
    "    bridge.command(\"setup\")\n",
    "    bridge.command(\"repeat \" + str(ini_mins) + \" [go]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Schedule_Gen(population, CV, day_num):\n",
    "    # generate population and schedules\n",
    "    if CV == 0:\n",
    "        CV = 0.000001\n",
    "    pop_num = range(population)\n",
    "    type_num = 1    # type of the person being generated: 1: full time employment\n",
    "    #day_num = 100   # number of days to be simulated\n",
    "\n",
    "    # assuming the following schedule sequence:\n",
    "    mu_Wmorning = 450 #time to go out (7:30)\n",
    "    sigma_Wmorning = mu_Wmorning*CV #standard deviation of morning trip\n",
    "    mu_Wnoon = 210 #duration of morning work\n",
    "    sigma_Wnoon = mu_Wnoon*CV #standard deviation of the duration of morning work\n",
    "    mu_L = 60 # duration of lunch break\n",
    "    sigma_L = mu_L*CV #standard deviation of the duration of lunch break\n",
    "    mu_Wafnoon = 270 #duration of afternoon work\n",
    "    sigma_Wafnoon = mu_Wafnoon*CV #standard deviation of the duration of morning work\n",
    "\n",
    "    trip_Wmorning = [int(x) for x in np.random.normal(mu_Wmorning, sigma_Wmorning, day_num)] #generate start time based on normal distribution\n",
    "    trip_Wnoon = [int(x) for x in np.random.normal(mu_Wnoon, sigma_Wnoon, day_num)] # work duration in the morning\n",
    "    trip_Wafnoon = [int(x) for x in np.random.normal(mu_Wafnoon, sigma_Wafnoon, day_num)] #work duration in the afternoon\n",
    "    trip_L = [int(x) for x in np.random.normal(mu_L, sigma_L, day_num)] #lunch break in the noon\n",
    "    trip_Smorning = [int(x) for x in np.random.normal(mu_L, sigma_L, day_num)] #school trip in the morning\n",
    "    trip_Safnoon = [int(x) for x in np.random.normal(mu_L, sigma_L, day_num)] #school trip in the afternoon\n",
    "\n",
    "    z_home = 'home'#15 #zone number for home\n",
    "    z_work = 'work'#25 #zone for work\n",
    "    z_shop = 'shopping'#35 #zone number for shopping\n",
    "    z_school = 'school'#45 #zone number for school\n",
    "\n",
    "    for y in pop_num:\n",
    "        demand_data = pd.DataFrame(None)\n",
    "        demand_data['id'] = range(day_num*1440) \n",
    "        demand_data['day_id'] = [int(x/ 1440) for x in demand_data['id']]\n",
    "        demand_data['minute_id'] = demand_data['id'] % 1440\n",
    "        base = datetime.datetime.strptime('2015-01-01 00:00',  '%Y-%m-%d %H:%M')\n",
    "        datetime_list = [base + datetime.timedelta(minutes=x) for x in range(0, day_num*1440)]\n",
    "        dow = [datetime.datetime.isoweekday(x) for x in datetime_list]\n",
    "        hr = [x.hour for x in datetime_list]\n",
    "        minu = [x.minute for x in datetime_list]\n",
    "        demand_data['datetime'] = [datetime.datetime.strftime(x, '%Y-%m-%d %H:%M') for x in datetime_list]\n",
    "        demand_data['dow'] = dow\n",
    "        demand_data['hour'] = hr\n",
    "        demand_data['minute'] = minu\n",
    "        demand_data['pid'] = y\n",
    "        demand_data['type'] = type_num\n",
    "        demand_data['zone'] = None\n",
    "        cur_day = None\n",
    "        uni_day = np.unique(demand_data['day_id'])\n",
    "        for i in uni_day:\n",
    "            cur_min = demand_data['minute_id'][i]\n",
    "            Sch_drop_S = trip_Wmorning[i]\n",
    "            Sch_drop_E = trip_Wmorning[i] + trip_Smorning[i]\n",
    "            Wrk_Mor_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i]\n",
    "            Shp_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i] + trip_L[i]\n",
    "            Wrk_Aft_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i] + trip_L[i] + trip_Wafnoon[i]\n",
    "            Sch_pik_E = trip_Wmorning[i] + trip_Smorning[i] + trip_Wnoon[i] + trip_L[i] + trip_Wafnoon[i] + trip_Safnoon[i]\n",
    "\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][0:Sch_drop_S], 'zone', z_home)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Sch_drop_S:Sch_drop_E], 'zone', z_school)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Sch_drop_E:Wrk_Mor_E], 'zone', z_work)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Wrk_Mor_E:Shp_E], 'zone', z_shop)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Shp_E:Wrk_Aft_E], 'zone', z_work)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Wrk_Aft_E:Sch_pik_E], 'zone', z_school)\n",
    "            demand_data.set_value(demand_data['id'][demand_data['day_id'] == i][Sch_pik_E:], 'zone', z_home)\n",
    "        demand_data['zone'].to_csv('ActivitySchedule_' + str(y) + '.csv', header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import simuated data\n",
    "# using function as the file can be closed automatically\n",
    "def rawimport(path, columnname):\n",
    "    agentdata = pd.read_csv(path, delimiter=' ', header=None, names=columnname)\n",
    "    agentdata = agentdata.drop(labels=\"todrop\", axis=1)\n",
    "    return agentdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run NL model; \n",
    "def run_NL(mob_call):\n",
    "    bridge.command(mob_call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_input(path_i, columnname_i):\n",
    "\n",
    "    agentdata = rawimport(path_i, columnname_i)\n",
    "    dt_stamp = [datetime.datetime.strptime(x,  '%Y-%m-%d %H:%M') for x in agentdata[\"acc_time\"]]\n",
    "    dow = [datetime.datetime.isoweekday(x) for x in dt_stamp]\n",
    "    hr = [x.hour for x in dt_stamp]\n",
    "    minu = [x.minute for x in dt_stamp]\n",
    "    agentdata['dow'] = dow\n",
    "    agentdata['hour'] = hr\n",
    "    agentdata['minute'] = minu\n",
    "\n",
    "    return (agentdata)\n",
    "\n",
    "#     mobiledata_ML = mobiledata.drop('radius', axis = 1)\n",
    "#     mobiledata_ML.drop('acc_time', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('acc_min', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('tid', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('txcor', axis=1, inplace=True)\n",
    "#     mobiledata_ML.drop('tycor', axis=1, inplace=True)\n",
    "    \n",
    "#     agentdata.drop('acc_time', axis=1, inplace=True)\n",
    "#     agentdata.drop('acc_min', axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "#    return (agentdata, mobiledata_ML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_adj (dataset, tobedropped):\n",
    "    \n",
    "    for i in tobedropped:\n",
    "        dataset.drop(i, axis=1, inplace=True)\n",
    "    \n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReturnedValue(object):\n",
    "    def __init__(self, clf1, clf2, clf3, clf4):\n",
    "        self.clf1 = clf1\n",
    "        self.clf2 = clf2\n",
    "        self.clf3 = clf3\n",
    "        self.clf4 = clf4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NL_ML(mobiledata_ML, day_new, traintest_pre):\n",
    "    # prepare for ML\n",
    "    \n",
    "    # need to keep previous training/testing samples, otherwise results flactuates\n",
    "    \n",
    "    mobiledata_new = mobiledata_ML[mobiledata_ML['day']>=day_new]\n",
    "    \n",
    "    X_new = mobiledata_new.drop('tzone', axis=1)\n",
    "    X_new = X_new.drop('day', axis=1)\n",
    "    y_new = mobiledata_new['tzone']\n",
    "    trainX_new, testX_new, trainY_new, testY_new = train_test_split(X_new, y_new, random_state = 10)\n",
    "        \n",
    "    trainX = pd.concat([traintest_pre[0], trainX_new])\n",
    "    testX = pd.concat([traintest_pre[1], testX_new])\n",
    "    trainY = pd.concat([traintest_pre[2], trainY_new])\n",
    "    testY = pd.concat([traintest_pre[3], testY_new])\n",
    "    \n",
    "    X = mobiledata_ML.drop('tzone', axis=1)\n",
    "    y = mobiledata_ML['tzone']\n",
    "    \n",
    "    # knn\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "    pred_knn = clf_knn.predict(testX)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    AC_knn = accuracy_score(testY, pred_knn)\n",
    "    RC_knn = recall_score(testY, pred_knn, average=None).mean()\n",
    "    CF_knn = mean_squared_error(testY, pred_knn)\n",
    "#    CV_knn = cross_val_score(clf_knn, X, y, cv=3)\n",
    "    ## using ridge regression\n",
    "    clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "    pred_rdg = clf_rdg.predict(testX)\n",
    "    AC_rdg = accuracy_score(testY, pred_rdg)\n",
    "    RC_rdg = recall_score(testY, pred_rdg, average=None).mean()  \n",
    "    CF_rdg =  mean_squared_error(testY, pred_rdg)\n",
    "#    CV_rdg = cross_val_score(clf_rdg, X, y, cv=3)\n",
    "    ## using RF\n",
    "    clf_rf = RandomForestClassifier(n_estimators=100, max_depth=100)\n",
    "    clf_rf.fit(trainX, trainY)\n",
    "    pred_rf = clf_rf.predict(testX)\n",
    "    AC_rf = accuracy_score(testY, pred_rf)\n",
    "    RC_rf = recall_score(testY, pred_rf, average=None).mean()   \n",
    "    CF_rf = mean_squared_error(testY, pred_rf)\n",
    "#    CV_rf = cross_val_score(clf_rf, X, y, cv=3)\n",
    "    ## using GBDT\n",
    "    clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "    clf_gb.fit(trainX, trainY)\n",
    "    pred_gb = clf_gb.predict(testX)\n",
    "    AC_gb = accuracy_score(testY, pred_gb)\n",
    "    RC_gb = recall_score(testY, pred_gb, average=None).mean()  \n",
    "    CF_gb = mean_squared_error(testY, pred_gb)\n",
    "#    CV_gb = cross_val_score(clf_gb, X, y, cv=3)\n",
    "\n",
    "    ##using neural networks\n",
    "    trainY_ = trainY.copy()\n",
    "    trainY_[trainY_==15] = 0\n",
    "    trainY_[trainY_==25] = 1\n",
    "    trainY_[trainY_==35] = 2\n",
    "    trainY_[trainY_==45] = 3\n",
    "    \n",
    "    pred, optimal_params = fit_predict_NN(trainX.values, trainY_.values, testX.values, [T.tanh]*2, [10, 5], 0.5)\n",
    "    #roc_auc_score(testY_, pred)\n",
    "    pred_ = pred.copy()\n",
    "    pred_[pred==0] = 15\n",
    "    pred_[pred==1] = 25\n",
    "    pred_[pred==2] = 35\n",
    "    pred_[pred==3] = 45\n",
    "    AC_ANN = accuracy_score(testY, pred_)\n",
    "    RC_ANN = recall_score(testY, pred_, average=None).mean()\n",
    "    CF_ANN = mean_squared_error(testY, pred_)\n",
    "    \n",
    "    trained_clf = [clf_knn, clf_rdg, clf_rf, clf_gb, optimal_params]\n",
    "#    trained_CV = [CV_knn, CV_rdg, CV_rf, CV_gb]\n",
    "    trained_RC = [RC_knn, RC_rdg, RC_rf, RC_gb, RC_ANN]\n",
    "    trained_CM = [CF_knn, CF_rdg, CF_rf, CF_gb, CF_ANN]\n",
    "    trained_AC = [AC_knn, AC_rdg, AC_rf, AC_gb, AC_ANN]\n",
    "    traintest = [trainX, testX, trainY, testY]\n",
    "    \n",
    "    return (trained_clf, trained_RC, trained_CM, trained_AC, traintest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_agent(trained_clf, agentdata, last_n, traintest_prev):\n",
    "    last_minutes = last_n * (-1440)\n",
    "    pred_X = pd.DataFrame(None)\n",
    "    pred_X['pid'] = agentdata['pid'][last_minutes:]\n",
    "    pred_X['tpid'] = 0\n",
    "    pred_X = pd.concat([pred_X, agentdata[last_minutes:].iloc[:,-3:]], axis = 1)\n",
    "\n",
    "#    pred_X['day'] = pred_X['day'] + 1\n",
    "#    pred_X['acc_time'] = pred_X['acc_time'] + 1440\n",
    "    pred_y = agentdata[last_minutes:].iloc[:,2]\n",
    "    \n",
    "\n",
    "    pred_knn = trained_clf[0].predict(pred_X)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    knn_AC = accuracy_score(pred_y, pred_knn)\n",
    "    knn_RC = recall_score(pred_y, pred_knn, average='macro')\n",
    "    knn_CM = mean_squared_error(pred_y, pred_knn)\n",
    "\n",
    "    ## using ridge regression\n",
    "    pred_rdg = trained_clf[1].predict(pred_X)\n",
    "    rdg_AC = accuracy_score(pred_y, pred_rdg)\n",
    "    rdg_RC = recall_score(pred_y, pred_rdg, average='macro')  \n",
    "    rdg_CM = mean_squared_error(pred_y, pred_rdg)\n",
    "\n",
    "    ## using RF\n",
    "    pred_rf = trained_clf[2].predict(pred_X)\n",
    "    rf_AC = accuracy_score(pred_y, pred_rf)\n",
    "    rf_RC = recall_score(pred_y, pred_rf, average='macro')  \n",
    "    rf_CM = mean_squared_error(pred_y, pred_rf)\n",
    "\n",
    "    ## using GBDT\n",
    "    pred_gb = trained_clf[3].predict(pred_X)\n",
    "    gb_AC = accuracy_score(pred_y, pred_gb)\n",
    "    gb_RC = recall_score(pred_y, pred_gb, average='macro')  \n",
    "    gb_CM = mean_squared_error(pred_y, pred_gb)\n",
    "    \n",
    "    ## using ANN\n",
    "    pred_ann = predict_NN(pred_X.values, traintest_prev[3], [T.tanh]*2, [10, 5], trained_clf[4])\n",
    "    pred_ann_ = pred_ann.copy()\n",
    "    pred_ann_[pred_ann==0] = 15\n",
    "    pred_ann_[pred_ann==1] = 25\n",
    "    pred_ann_[pred_ann==2] = 35\n",
    "    pred_ann_[pred_ann==3] = 45\n",
    "    AC_ANN = accuracy_score(pred_y, pred_ann_)\n",
    "    RC_ANN = recall_score(pred_y, pred_ann_, average='macro')\n",
    "    CF_ANN = mean_squared_error(pred_y, pred_ann_)\n",
    "    \n",
    "    pr_RC = [knn_RC, rdg_RC, rf_RC, gb_RC, RC_ANN]\n",
    "    pr_CM = [knn_CM, rdg_CM, rf_CM, gb_CM, CF_ANN]\n",
    "    pr_AC = [knn_AC, rdg_AC, rf_AC, gb_AC, AC_ANN]\n",
    "    pr_data = [pred_X, pred_y]\n",
    "\n",
    "    return (pr_RC, pr_CM, pr_AC, pr_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANN - prediction only\n",
    "def predict_NN(testX, train_y, activate_functions, hidden_layers, fittedpar):\n",
    "    dim = testX.shape[1]\n",
    "    dim_y = len(np.unique(train_y))\n",
    "    param = T.vector()\n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        #going through hidden layers\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        N = dim_previous * dim_y\n",
    "        v_ = parameter[n_previous:n_previous + N].reshape((dim_previous, dim_y))\n",
    "        output = h.dot(v_)\n",
    "        n_previous += N\n",
    "        return T.nnet.softmax(output), n_previous\n",
    "    \n",
    "    data = T.matrix()\n",
    "#    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    prediction = T.argmax(activation(data, param)[0], axis=1)\n",
    "    predict = theano.function([data, param], prediction)\n",
    "    return predict(testX, fittedpar)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# artificial neural networks\n",
    "def fit_predict_NN(trainX, trainY, testX, activate_functions, hidden_layers, reg_lambda):\n",
    "    X_ = theano.shared(trainX, name='X')\n",
    "    y_ = theano.shared(trainY, name='y')\n",
    "    param = T.vector()\n",
    "    dim = trainX.shape[1]\n",
    "    num_examples = trainX.shape[0]\n",
    "    dim_y = len(np.unique(trainY))\n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        #going through hidden layers\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        N = dim_previous * dim_y\n",
    "        v_ = parameter[n_previous:n_previous + N].reshape((dim_previous, dim_y))\n",
    "        output = h.dot(v_)\n",
    "        n_previous += N\n",
    "        return T.nnet.softmax(output), n_previous\n",
    "\n",
    "    y_hat, dim_W = activation(X_, param)\n",
    "    # optimize\n",
    "    #regularisation\n",
    "    loss_reg = 1./num_examples * reg_lambda/2 * T.sum(T.sqr(param)) \n",
    "    #cost\n",
    "    loss = T.nnet.categorical_crossentropy(y_hat, y_).mean() + loss_reg\n",
    "    loss_function = theano.function([param], loss)\n",
    "    loss_grad = theano.function([param], theano.grad(loss, param))\n",
    "    \n",
    "    result = minimize(loss_function, jac=loss_grad, x0=np.random.normal(size=activation(X_, param)[1]))\n",
    "    optimal_params = result['x']\n",
    "#    print result\n",
    "#    forward_prop = theano.function([param], y_hat)\n",
    "#    theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile='img/nn-theano-forward_prop.png', format='png')\n",
    "#    SVG(theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, return_image=True, format='svg'))\n",
    "\n",
    "    # predict data\n",
    "    data = T.matrix()\n",
    "#    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    prediction = T.argmax(activation(data, param)[0], axis=1)\n",
    "    predict = theano.function([data, param], prediction)\n",
    "    return (predict(testX, optimal_params), optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "1\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "2\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "3\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "4\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "5\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "6\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "7\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "8\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "9\n",
      "The output file is available at img/nn-theano-forward_prop.png\n",
      "10\n",
      "The output file is available at img/nn-theano-forward_prop.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-4a84db8e1a06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         train_classifier, train_RC, train_CM, train_AC, traintestsplit = NL_ML(mobiledata_ML, currentday,\n\u001b[1;32m---> 68\u001b[1;33m                                                                                         traintestsplit)\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mrun_NL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmob_call\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-18c0eca35fcd>\u001b[0m in \u001b[0;36mNL_ML\u001b[1;34m(mobiledata_ML, day_new, traintest_pre)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mtrainY_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainY_\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimal_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_predict_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;31m#roc_auc_score(testY_, pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mpred_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-e042b27fe0f0>\u001b[0m in \u001b[0;36mfit_predict_NN\u001b[1;34m(trainX, trainY, testX, activate_functions, hidden_layers, reg_lambda)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mforward_prop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprinting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpydotprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_with_name_simple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'img/nn-theano-forward_prop.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprinting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpydotprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_with_name_simple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# predict data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\tdm-gcc-64\\theano-rel-0.8.2\\theano\\printing.pyc\u001b[0m in \u001b[0;36mpydotprint\u001b[1;34m(fct, outfile, compact, format, with_ids, high_contrast, cond_highlight, colorCodes, max_label_size, scan_graphs, var_with_name_simple, print_output_file, return_image)\u001b[0m\n\u001b[0;32m    938\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_label_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'...'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                 \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEdge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapply_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pydot.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, dst, obj_dict, **attrs)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'points'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_attribute_methods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEDGE_ATTRIBUTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\pydot.pyc\u001b[0m in \u001b[0;36mcreate_attribute_methods\u001b[1;34m(self, obj_attributes)\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;31m# Generate all the Setter methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m             \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'set_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attributes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;31m# Generate all the Getter methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## run Netlogo and carryout ML\n",
    "\n",
    "agent_num = 2 #define the number of agents to be simulated\n",
    "counter = 0\n",
    "ini_day = 0 # define the number of days to initiate the model (i.e. the original training data)\n",
    "\n",
    "last_n = 1 # define the number of days to be left out from simulation data for testing purpose\n",
    "tot_day = 100 # define the number of days to be simulated\n",
    "sim_day = 1 # define for one simulation how many days to be simulated\n",
    "#num_sim = 1440 # define the duration of simulation (in minutes)\n",
    "CV_range = np.arange(0, 0.51, 0.02)\n",
    "\n",
    "# setting data storage\n",
    "Results_sum = np.zeros((len(CV_range), 2, 3, 4, tot_day)) # CV, TR/PR, RC/CM/AC, CLF, Days\n",
    "agentdata_tot = {}\n",
    "mobiledata_tot = {}\n",
    "smdata_tot = {}\n",
    "traintestsplit_tot = {}\n",
    "pr_data_tot = {}\n",
    "# agentdata_tot = np.zeros((len(CV_range), tot_day)) \n",
    "# mobiledata_tot = np.zeros((len(CV_range), tot_day)) \n",
    "# traintestsplit_tot = np.zeros((len(CV_range), tot_day))\n",
    "# pr_data_tot = np.zeros((len(CV_range), tot_day)) \n",
    "\n",
    "# generating demand profile\n",
    "for y in CV_range:\n",
    "    print y\n",
    "    Schedule_Gen(agent_num, y, tot_day + 1)\n",
    "\n",
    "    ## initiating the model, \n",
    "    model_init(ini_day*1440)\n",
    "    traintestsplit = [None, None, None, None]\n",
    "    \n",
    "    sim_min = sim_day*1440\n",
    "    mob_call = \"repeat \" + str(sim_min) + \" [go]\" \n",
    "    run_NL(mob_call)\n",
    "    \n",
    "    path = './agentdata.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "    agentdata  = data_input(path, columnname)\n",
    "    \n",
    "    path = './mobiletowers.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "    mobiledata_ML = data_input(path, columnname)\n",
    "    \n",
    "    path = './socialmedia.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"smpid\", \"smzone\", \"smxcor\", \"smycor\"]\n",
    "    smdata_ML = data_input(path, columnname)\n",
    "    \n",
    "    todrop = [\"acc_time\", \"acc_min\"]\n",
    "    agentdata = data_adj(agentdata, todrop)\n",
    "    \n",
    "    todrop = [\"radius\", \"acc_time\", \"acc_min\", \"tid\", \"txcor\", \"tycor\"]\n",
    "    mobiledata_ML = data_adj(mobiledata_ML, todrop)\n",
    "    \n",
    "    todrop = [\"acc_time\", \"acc_min\", \"smxcor\", \"smycor\"]\n",
    "    smdata_ML = data_adj(smdata_ML, todrop)\n",
    "    \n",
    "    agentdata_tot[counter, 0] = agentdata\n",
    "    mobiledata_tot[counter, 0] = mobiledata_ML\n",
    "    smdata_tot[counter, 0] = smdata_ML\n",
    "    ## loop + prediction\n",
    "    for x in range(0, tot_day, sim_day):\n",
    "        print x\n",
    "        currentday = x\n",
    "        \n",
    "        train_classifier, train_RC, train_CM, train_AC, traintestsplit = NL_ML(mobiledata_ML, currentday,\n",
    "                                                                                        traintestsplit)\n",
    "        \n",
    "        run_NL(mob_call)\n",
    "        \n",
    "        path = './agentdata.txt'\n",
    "        columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "        agentdata  = data_input(path, columnname)\n",
    "\n",
    "        path = './mobiletowers.txt'\n",
    "        columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "        mobiledata_ML = data_input(path, columnname)\n",
    "\n",
    "        path = './socialmedia.txt'\n",
    "        columnname = [\"todrop\", \"acc_time\", \"day\", \"acc_min\", \"pid\", \"smpid\", \"smzone\", \"smxcor\", \"smycor\"]\n",
    "        smdata_ML = data_input(path, columnname)\n",
    "\n",
    "        todrop = [\"acc_time\", \"acc_min\"]\n",
    "        agentdata = data_adj(agentdata, todrop)\n",
    "\n",
    "        todrop = [\"radius\", \"acc_time\", \"acc_min\", \"tid\", \"txcor\", \"tycor\"]\n",
    "        mobiledata_ML = data_adj(mobiledata_ML, todrop)\n",
    "        \n",
    "        todrop = [\"acc_time\", \"acc_min\", \"smxcor\", \"smycor\"]\n",
    "        smdata_ML = data_adj(smdata_ML, todrop)\n",
    "\n",
    "#         agentdata_tot[counter, 0] = agentdata\n",
    "#         mobiledata_tot[counter, 0] = mobiledata_ML\n",
    "#         smdata_tot[counter, 0] = smdata_ML\n",
    "        \n",
    "        pred_RC, pred_CM, pred_AC, pred_data = pred_agent(train_classifier, agentdata, last_n, traintestsplit)\n",
    "        \n",
    "        \n",
    "        agentdata_tot[counter, x+1] = agentdata\n",
    "        mobiledata_tot[counter, x+1] = mobiledata_ML\n",
    "        smdata_tot[counter, x+1] = smdata_ML\n",
    "        traintestsplit_tot[counter, x] = traintestsplit\n",
    "        pr_data_tot[counter, x] = pred_data\n",
    "        \n",
    "        Results_sum[counter, 0, 0, 0, x] = train_RC[0] # CV, TR, RC, KNN, days\n",
    "        Results_sum[counter, 0, 0, 1, x] = train_RC[1] # CV, TR, RC, rdg, days\n",
    "        Results_sum[counter, 0, 0, 2, x] = train_RC[2] # CV, TR, RC, rf, days\n",
    "        Results_sum[counter, 0, 0, 3, x] = train_RC[3] # CV, TR, RC, gb, days\n",
    "        \n",
    "        Results_sum[counter, 0, 1, 0, x] = train_AC[0] # CV, TR, AC, KNN, days\n",
    "        Results_sum[counter, 0, 1, 1, x] = train_AC[1] # CV, TR, AC, rdg, days\n",
    "        Results_sum[counter, 0, 1, 2, x] = train_AC[2] # CV, TR, AC, rf, days\n",
    "        Results_sum[counter, 0, 1, 3, x] = train_AC[3] # CV, TR, AC, gb, days\n",
    "        \n",
    "        Results_sum[counter, 0, 2, 0, x] = train_CM[0] # CV, TR, CM, KNN, days\n",
    "        Results_sum[counter, 0, 2, 1, x] = train_CM[1] # CV, TR, CM, rdg, days\n",
    "        Results_sum[counter, 0, 2, 2, x] = train_CM[2] # CV, TR, CM, rf, days\n",
    "        Results_sum[counter, 0, 2, 3, x] = train_CM[3] # CV, TR, CM, gb, days\n",
    "        \n",
    "        Results_sum[counter, 1, 0, 0, x] = pred_RC[0] # CV, PR, RC, KNN, days\n",
    "        Results_sum[counter, 1, 0, 1, x] = pred_RC[1] # CV, PR, RC, rdg, days\n",
    "        Results_sum[counter, 1, 0, 2, x] = pred_RC[2] # CV, PR, RC, rf, days\n",
    "        Results_sum[counter, 1, 0, 3, x] = pred_RC[3] # CV, PR, RC, gb, days\n",
    "        \n",
    "        Results_sum[counter, 1, 1, 0, x] = pred_AC[0] # CV, PR, AC, KNN, days\n",
    "        Results_sum[counter, 1, 1, 1, x] = pred_AC[1] # CV, PR, AC, rdg, days\n",
    "        Results_sum[counter, 1, 1, 2, x] = pred_AC[2] # CV, PR, AC, rf, days\n",
    "        Results_sum[counter, 1, 1, 3, x] = pred_AC[3] # CV, PR, AC, gb, days\n",
    "        \n",
    "        Results_sum[counter, 1, 2, 0, x] = pred_CM[0] # CV, PR, CM, KNN, days\n",
    "        Results_sum[counter, 1, 2, 1, x] = pred_CM[1] # CV, PR, CM, rdg, days\n",
    "        Results_sum[counter, 1, 2, 2, x] = pred_CM[2] # CV, PR, CM, rf, days\n",
    "        Results_sum[counter, 1, 2, 3, x] = pred_CM[3] # CV, PR, CM, gb, days\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exception_verbosity='high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainY_ = trainY.copy()\n",
    "trainY_[trainY_==15] = 0\n",
    "trainY_[trainY_==25] = 1\n",
    "trainY_[trainY_==35] = 2\n",
    "trainY_[trainY_==45] = 3\n",
    "trainY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = trainX[:, 0].min() - .5, trainX[:, 0].max() + .5\n",
    "    y_min, y_max = trainX[:, 1].min() - .5, trainX[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(trainX[:, 0], trainX[:, 1], c=trainY, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## using Neural Network\n",
    "# Size definitions\n",
    "num_examples = len(trainX) # training set size\n",
    "nn_input_dim = 5 # input layer dimensionality\n",
    "nn_output_dim = 4 # output layer dimensionality\n",
    "nn_hdim = 100 # hiden layer dimensionality\n",
    "\n",
    "# Our data vectors\n",
    "X = T.matrix('X') # matrix of doubles\n",
    "y = T.lvector('y') # vector of int64\n",
    "\n",
    "# Gradient descent parameters\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength \n",
    "\n",
    "# Shared variables with initial values. We need to learn these.\n",
    "W1 = theano.shared(np.random.randn(nn_input_dim, nn_hdim), name='W1')\n",
    "b1 = theano.shared(np.zeros(nn_hdim), name='b1')\n",
    "W2 = theano.shared(np.random.randn(nn_hdim, nn_output_dim), name='W2')\n",
    "b2 = theano.shared(np.zeros(nn_output_dim), name='b2')\n",
    "\n",
    "# Forward propagation\n",
    "# Note: We are just defining the expressions, nothing is evaluated here!\n",
    "z1 = X.dot(W1) + b1\n",
    "a1 = T.tanh(z1)\n",
    "z2 = a1.dot(W2) + b2\n",
    "y_hat = T.nnet.softmax(z2) # output probabilties\n",
    "\n",
    "# The regularization term (optional)\n",
    "loss_reg = 1./num_examples * reg_lambda/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2))) \n",
    "# the loss function we want to optimize\n",
    "\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean() #+ loss_reg\n",
    "\n",
    "# Returns a class prediction\n",
    "prediction = T.argmax(y_hat, axis=1)\n",
    "\n",
    "# Theano functions that can be called from our Python code\n",
    "forward_prop = theano.function([X], y_hat)\n",
    "calculate_loss = theano.function([X, y], loss)\n",
    "predict = theano.function([X], prediction)\n",
    "\n",
    "# Easy: Let Theano calculate the derivatives for us!\n",
    "dW2 = T.grad(loss, W2)\n",
    "db2 = T.grad(loss, b2)\n",
    "dW1 = T.grad(loss, W1)\n",
    "db1 = T.grad(loss, b1)\n",
    "\n",
    "gradient_step = theano.function(\n",
    "    [X, y],\n",
    "    updates=((W2, W2 - epsilon * dW2),\n",
    "             (W1, W1 - epsilon * dW1),\n",
    "             (b2, b2 - epsilon * db2),\n",
    "             (b1, b1 - epsilon * db1)))\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "build_model(print_loss=True)\n",
    "\n",
    "#prediction\n",
    "pred = predict(testX.values)\n",
    "\n",
    "# # Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: predict(x))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(num_passes=20000, print_loss=False):\n",
    "    \n",
    "    # Re-Initialize the parameters to random values. We need to learn these.\n",
    "    # (Needed in case we call this function multiple times)\n",
    "    np.random.seed=10\n",
    "    W1.set_value(np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim))\n",
    "    b1.set_value(np.zeros(nn_hdim))\n",
    "    W2.set_value(np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim))\n",
    "    b2.set_value(np.zeros(nn_output_dim))\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in xrange(0, num_passes):\n",
    "        # This will update our parameters W2, b2, W1 and b1!\n",
    "        gradient_step(trainX.values, trainY_.values)\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "            print \"Loss after iteration %i: %f\" %(i, calculate_loss(trainX.values, trainY_.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.gca(projection='3d')\n",
    "X = range(0,26)\n",
    "Y = range(0,100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = Results_sum[X, 1, 0, 3, Y]\n",
    "surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "vals = ax.get_xticks()\n",
    "ax.set_xticklabels(CV_range[[x for x in vals]])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(Results_sum[25, 1, 0, 0, :], label='KNN', color=\"g\")\n",
    "ax.plot(Results_sum[25, 1, 0, 1, :], label='Ridge', color=\"b\")\n",
    "ax.plot(Results_sum[25, 1, 0, 2, :], label='RF', color=\"y\")\n",
    "ax.plot(Results_sum[25, 1, 0, 3, :], label='GBDT', color=\"r\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of days\")\n",
    "ax.set_xticks(range(0, len(Results_sum[0, 1, 0, 0, :]), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(knn_all_AC_tr, label='KNN', color=\"g\")\n",
    "ax.plot(rdg_all_AC_tr, label='Ridge', color=\"b\")\n",
    "ax.plot(rf_all_AC_tr, label='RF', color=\"y\")\n",
    "ax.plot(gb_all_AC_tr, label='GBDT', color=\"r\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of simulated days\")\n",
    "ax.set_xticks(range(0, len(knn_all_AC_tr), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(knn_all_RC, label='KNN', color=\"g\")\n",
    "ax.plot(rdg_all_RC, label='Ridge', color=\"b\")\n",
    "ax.plot(rf_all_RC, label='RF', color=\"y\")\n",
    "ax.plot(gb_all_RC, label='GBDT', color=\"r\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of simulated days\")\n",
    "ax.set_xticks(range(0, len(knn_all_RC), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(knn_all_RC_tr, label='KNN', color=\"g\")\n",
    "ax.plot(rdg_all_RC_tr, label='Ridge', color=\"b\")\n",
    "ax.plot(rf_all_RC_tr, label='RF', color=\"y\")\n",
    "ax.plot(gb_all_RC_tr, label='GBDT', color=\"r\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:3.0f}%'.format(x*100) for x in vals])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Number of simulated days\")\n",
    "ax.set_xticks(range(0, len(knn_all_RC_tr), 10))\n",
    "ax.set_xticklabels(range(ini_day, tot_day + 1, 10))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobiledata_PL = mobiledata_ML.copy()\n",
    "mobiledata_PL['min_acc'] = mobiledata_PL['hour'] * 60 + mobiledata_PL['minute']\n",
    "fig1 = plt.figure(figsize=(10,5))\n",
    "ax = fig1.add_subplot(1, 1, 1)\n",
    "ax.scatter(mobiledata_PL['min_acc'], mobiledata_PL['tzone'], marker = \"|\")\n",
    "ax.set_xticks(range(0, 1441, 1440/12))\n",
    "ax.set_yticks(range(0, 46, 15))\n",
    "vals = ax.get_xticks()\n",
    "print vals\n",
    "ax.set_xticklabels(['{:02.0f}h'.format(x/60) for x in vals])\n",
    "ax.set_ylabel(\"Zone number\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "# ax2.plot(mobiledata_sel['minute'], mobiledata_sel['tzone_y'])\n",
    "# ax2.set_xticks(range(0, 1440, 200))\n",
    "# ax2.set_yticks(range(0, 45, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checking the distribution of mobile phone calls\n",
    "fig2 = plt.figure(figsize=(10,5))\n",
    "ax = fig2.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "(mobiledata_PL['min_acc'][pd.notnull(mobiledata_PL['tzone'])]).hist(bins=12)\n",
    "ax.set_xticks(range(0, 1441, 1440/12))\n",
    "vals = ax.get_xticks()\n",
    "print vals\n",
    "ax.set_xticklabels(['{:02.0f}h'.format(x/60) for x in vals])\n",
    "ax.set_ylabel(\"Number of calls (accumulated)\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "#ax.set_xticks(range(0, 1440, 1440/12))\n",
    "#plt.hist(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobiledata_PL = mobiledata_ML.copy()\n",
    "mobiledata_PL['min_acc'] = mobiledata_PL['hour'] * 60 + mobiledata_PL['minute']\n",
    "print mobiledata_PL.max()\n",
    "print mobiledata_PL.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "A = np.random.rand(1000,10000).astype(theano.config.floatX)\n",
    "B = np.random.rand(10000,1000).astype(theano.config.floatX)\n",
    "np_start = time.time()\n",
    "AB = A.dot(B)\n",
    "np_end = time.time()\n",
    "X,Y = theano.tensor.matrices('XY')\n",
    "mf = theano.function([X,Y],X.dot(Y))\n",
    "t_start = time.time()\n",
    "tAB = mf(A,B)\n",
    "t_end = time.time()\n",
    "print(\"NP time: %f[s], theano time: %f[s] (times should be close when run on CPU!)\" %(\n",
    "                                           np_end-np_start, t_end-t_start))\n",
    "print(\"Result difference: %f\" % (np.abs(AB-tAB).max(), ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print aa[aa['minute']==789], bb[bb['minute']==789]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class User(object):\n",
    "    def __init__(self, name, email):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "    def commit(self):\n",
    "        pass\n",
    "\n",
    "jason = User('jason', 'jason@email.com')\n",
    "jack = User('jack', 'jack@yahoo.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    user_paths = os.environ['PYTHONPATH'].split(os.pathsep)\n",
    "except KeyError:\n",
    "    user_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = traintestsplit_tot[99]\n",
    "pred_X, pred_y = pr_data_tot[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainX.drop('acc_time', axis=1, inplace=True)\n",
    "trainX.drop('day', axis=1, inplace=True)\n",
    "#testX.drop('acc_time', axis=1, inplace=True)\n",
    "testX.drop('day', axis=1, inplace=True)\n",
    "#pred_X.drop('acc_time', axis=1, inplace=True)\n",
    "pred_X.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## using RF\n",
    "clf_rf = RandomForestClassifier()#n_estimators=1000, max_depth=100)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "print precision_score(testY, pred_rf, average=None)\n",
    "print accuracy_score(testY, pred_rf)\n",
    "print recall_score(testY, pred_rf, average='macro')\n",
    "print recall_score(testY, pred_rf, average='micro')\n",
    "print confusion_matrix(testY, pred_rf)\n",
    "\n",
    "pred_rf = clf_rf.predict(pred_X)\n",
    "print precision_score(pred_y, pred_rf, average=None)\n",
    "print accuracy_score(pred_y, pred_rf)\n",
    "print recall_score(pred_y, pred_rf, average='macro')\n",
    "print recall_score(pred_y, pred_rf, average='micro')\n",
    "print confusion_matrix(pred_y, pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average=None)\n",
    "print accuracy_score(testY, pred_knn)\n",
    "print recall_score(testY, pred_knn, average='macro')\n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "print confusion_matrix(testY, pred_knn)\n",
    "\n",
    "pred_knn = clf_knn.predict(pred_X)\n",
    "print precision_score(pred_y, pred_knn, average=None)\n",
    "print accuracy_score(pred_y, pred_knn)\n",
    "print recall_score(pred_y, pred_knn, average='macro')\n",
    "print recall_score(pred_y, pred_knn, average='micro')\n",
    "print confusion_matrix(pred_y, pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# knn\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average=None)\n",
    "print accuracy_score(testY, pred_knn)\n",
    "print recall_score(testY, pred_knn, average='macro')\n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "print confusion_matrix(testY, pred_knn)\n",
    "\n",
    "pred_knn = clf_knn.predict(pred_X)\n",
    "print precision_score(pred_y, pred_knn, average=None)\n",
    "print accuracy_score(pred_y, pred_knn)\n",
    "print recall_score(pred_y, pred_knn, average='macro')\n",
    "print recall_score(pred_y, pred_knn, average='micro')\n",
    "print confusion_matrix(pred_y, pred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './agentdata.txt'\n",
    "columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "agentdata = rawimport(path, columnname)\n",
    "dt_stamp = [datetime.strptime(x,  '%Y-%m-%d %H:%M') for x in agentdata[\"acc_time\"]]\n",
    "dow = [datetime.isoweekday(x) for x in dt_stamp]\n",
    "hr = [x.hour for x in dt_stamp]\n",
    "minu = [x.minute for x in dt_stamp]\n",
    "agentdata['dow'] = dow\n",
    "agentdata['hour'] = hr\n",
    "agentdata['minute'] = minu\n",
    "\n",
    "\n",
    "path = './mobiletowers.txt'\n",
    "columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "mobiledata = rawimport(path, columnname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_stamp = []\n",
    "for x in agentdata[\"acc_time\"].values:\n",
    "    dt_stamp.append(datetime.strptime(x, '%Y-%m-%d %H:%M'))\n",
    "dt_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'agentdata['hour'] = hr\n",
    "agentdata['minute'] = minu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for x in agentdata[\"acc_time\"].values:\n",
    "    print parse(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leave the last n day(s) out for testing purpose\n",
    "\n",
    "#mobiledata_ML = mobiledata[mobiledata['day'] <= (mobiledata['day'].max() - last_n)]\n",
    "mobiledata_ML = mobiledata.drop('radius', axis = 1)\n",
    "mobiledata_ML.drop('tid', axis=1, inplace=True)\n",
    "mobiledata_ML.drop('txcor', axis=1, inplace=True)\n",
    "mobiledata_ML.drop('tycor', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare for ML\n",
    "X = mobiledata_ML.drop('tzone', axis=1)\n",
    "y = mobiledata_ML['tzone']\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k fold cross-validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "for train_indices, test_indices in KFold(len(X), n_folds=3):\n",
    "    fold_trainX = X.iloc[train_indices, :]\n",
    "    fold_testX  = X.iloc[test_indices, :]\n",
    "    fold_trainY = y[train_indices]\n",
    "    fold_testY  = y[test_indices]\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=2).fit(fold_trainX, fold_trainY)\n",
    "    pred_knn = clf_knn.predict(fold_testX)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    print precision_score(fold_testY, pred_knn, average='micro')  \n",
    "    print recall_score(fold_testY, pred_knn, average='micro')\n",
    "    confusion_matrix(fold_testY, pred_knn)\n",
    "    #print fold_trainY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# knn\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average=None)\n",
    "print accuracy_score(testY, pred_knn)\n",
    "print recall_score(testY, pred_knn, average='macro')\n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "print confusion_matrix(testY, pred_knn)\n",
    "cross_val_score(clf_knn, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using ridge regression\n",
    "clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "pred_rdg = clf_rdg.predict(testX)\n",
    "print precision_score(testY, pred_rdg, average='micro')  \n",
    "print confusion_matrix(testY, pred_rdg)\n",
    "cross_val_score(clf_rdg, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using RF\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=100)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "print precision_score(testY, pred_rf, average='micro')  \n",
    "print confusion_matrix(testY, pred_rf)\n",
    "cross_val_score(clf_rf, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using GBDT\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "clf_gb.fit(trainX, trainY)\n",
    "pred_gb = clf_gb.predict(testX)\n",
    "print precision_score(testY, pred_gb, average='micro')  \n",
    "print confusion_matrix(testY, pred_gb)\n",
    "cross_val_score(clf_gb, X, y, cv=3)\n",
    "#test_qualities = []\n",
    "# for p in clf_gb.staged_predict_proba(testX):\n",
    "#     test_qualities.append(precision_score(testY, p[:, 1]))\n",
    "# plt.plot(test_qualities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_n = 1\n",
    "last_minutes = last_n * (-1440)\n",
    "pred_X = agentdata[last_minutes:].iloc[:,0:4]\n",
    "pred_X['tpid'] = 0\n",
    "pred_y = agentdata[last_minutes:].iloc[:,4]\n",
    "\n",
    "# pred_knn = trained_clf.clf1.predict(pred_X)\n",
    "# #print roc_auc_score(testY, pred_knn)\n",
    "# knn_AC = accuracy_score(pred_y, pred_knn)\n",
    "# knn_RC = recall_score(pred_y, pred_knn, average='macro')\n",
    "# knn_CM = confusion_matrix(pred_y, pred_knn)\n",
    "\n",
    "# ## using ridge regression\n",
    "# pred_rdg = trained_clf.clf2.predict(pred_X)\n",
    "# rdg_AC = accuracy_score(pred_y, pred_rdg)\n",
    "# rdg_RC = recall_score(pred_y, pred_rdg, average='macro')  \n",
    "# rdg_CM = confusion_matrix(pred_y, pred_rdg)\n",
    "\n",
    "# ## using RF\n",
    "# pred_rf = trained_clf.clf3.predict(pred_X)\n",
    "# rf_AC = accuracy_score(pred_y, pred_rf)\n",
    "# rf_RC = recall_score(pred_y, pred_rf, average='macro')  \n",
    "# rf_CM = confusion_matrix(pred_y, pred_rf)\n",
    "\n",
    "## using GBDT\n",
    "pred_gb = clf_gb.predict(pred_X)\n",
    "gb_AC = accuracy_score(pred_y, pred_gb)\n",
    "gb_RC = recall_score(pred_y, pred_gb, average='macro')  \n",
    "gb_CM = confusion_matrix(pred_y, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using SVM\n",
    "## solving multi-classification problem?\n",
    "clf_svc = SVC()\n",
    "clf_svc.fit(trainX, trainY)\n",
    "pred_svc = clf_svc.predict(testX)\n",
    "print precision_score(testY, pred_svc, average='micro')  \n",
    "print confusion_matrix(testY, pred_svc)\n",
    "cross_val_score(clf_svc, X, y, cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## prediction\n",
    "# pre_agent1 = mobiledata[:1]\n",
    "# pre_agent = pd.concat([pre_agent1]*1440)\n",
    "# pre_agent['day']=mobiledata['day'].max()+1\n",
    "# pre_agent['minute']=range(0,1440)\n",
    "# pre_agent['acc_time']=pre_agent['minute'] + pre_agent['day']*1440\n",
    "# pre_agent.drop('tzone', axis=1, inplace=True)\n",
    "# pre_agent.drop('radius', axis=1, inplace=True)\n",
    "# pre_agent.drop('tid', axis=1, inplace=True)\n",
    "# pre_agent.drop('txcor', axis=1, inplace=True)\n",
    "# pre_agent.drop('tycor', axis=1, inplace=True)\n",
    "pred_X = agentdata[-1440:].iloc[:,0:4]\n",
    "pred_X['tpid'] = 0\n",
    "pred_y = agentdata[-1440:].iloc[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_knn = clf_knn.predict(pred_X)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "knn_PR = precision_score(pred_y, pred_knn, average='micro')\n",
    "knn_CM = confusion_matrix(pred_y, pred_knn)\n",
    "\n",
    "## using ridge regression\n",
    "pred_rdg = clf_rdg.predict(pred_X)\n",
    "rdg_PR = precision_score(pred_y, pred_rdg, average='micro')  \n",
    "rdg_CM = confusion_matrix(pred_y, pred_rdg)\n",
    "\n",
    "## using RF\n",
    "pred_rf = clf_rf.predict(pred_X)\n",
    "rf_PR = precision_score(pred_y, pred_rf, average='micro')  \n",
    "rf_CM = confusion_matrix(pred_y, pred_rf)\n",
    "\n",
    "## using GBDT\n",
    "pred_gb = clf_gb.predict(pred_X)\n",
    "gb_PR = precision_score(pred_y, pred_gb, average='micro')  \n",
    "gb_CM = confusion_matrix(pred_y, pred_gb)\n",
    "\n",
    "print knn_PR, rdg_PR, rf_PR, gb_PR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_gb\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare for ML\n",
    "X = mobiledata_fs_filled.drop('zone', axis=1)\n",
    "y = mobiledata_fs_filled['zone']\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)\n",
    "\n",
    "## using KNN\n",
    "# weights might need to be changed - more weights towards the recent data?\n",
    "# metrics? canberra, minkowski\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "knn_PR = precision_score(testY, pred_knn, average='micro')\n",
    "knn_CM = confusion_matrix(testY, pred_knn)\n",
    "\n",
    "## using ridge regression\n",
    "clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "pred_rdg = clf_rdg.predict(testX)\n",
    "rdg_PR = precision_score(testY, pred_rdg, average='micro')  \n",
    "rdg_CM = confusion_matrix(testY, pred_rdg)\n",
    "\n",
    "## using RF\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "rf_PR = precision_score(testY, pred_rf, average='micro')  \n",
    "rf_CM = confusion_matrix(testY, pred_rf)\n",
    "\n",
    "## using GBDT\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "clf_gb.fit(trainX, trainY)\n",
    "pred_gb = clf_gb.predict(testX)\n",
    "gb_PR = precision_score(testY, pred_gb, average='micro')  \n",
    "gb_CM = confusion_matrix(testY, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class ReturnValue(object):\n",
    "#    def __init__(self, y0, y1):\n",
    "#        self.y0 = y0\n",
    "#        self.y1 = y1\n",
    "\n",
    "def data_input():\n",
    "    path = './agentdata.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"zone\", \"status\", \"xcor\", \"ycor\", \"tid\"]\n",
    "    agentdata = rawimport(path, columnname)\n",
    "    \n",
    "    path = './mobiletowers.txt'\n",
    "    columnname = [\"todrop\", \"acc_time\", \"day\", \"minute\", \"pid\", \"tpid\", \"tzone\", \"radius\", \"tid\", \"txcor\", \"tycor\"]\n",
    "    mobiledata = rawimport(path, columnname)\n",
    "    \n",
    "    # extend data to full (missing data shown as NaN)\n",
    "    mobiledata = mobiledata.drop(labels = [\"tid\"], axis=1)\n",
    "\n",
    "    mobiledata_fs = pd.merge(agentdata, mobiledata, how = 'left', on = [\"acc_time\", 'day', 'minute', \"pid\"])\n",
    "    #mobiledata_fs = pd.merge(agentdata_ri, mobiledata_ri, how = 'left', left_index = True, right_index = True)\n",
    "    #print mobiledata_fs.count()\n",
    "    #mobiledata_fs\n",
    "# fill the missing data using NN (k=1)\n",
    "# the reason why \"day\" and \"minute\" need to be removed before imputation is that the imputation is done based on features.\n",
    "# in other words, the day and minute will have impacts on the imputed results\n",
    "# to maintain a continuity of a user between days it is necessary to remove \"day\" and \"minute\" before imputation and add them back later\n",
    "# also no need to imputate tower id\n",
    "    mobiledata_fs_test = mobiledata_fs.drop(labels = [\"day\"], axis=1)\n",
    "    mobiledata_fs_test = mobiledata_fs_test.drop(labels = [\"minute\"], axis=1)\n",
    "    mobiledata_fs_test = mobiledata_fs_test.drop(labels = [\"tid\"], axis=1)\n",
    "    mobiledata_fs_filled = KNN(k=1).complete(mobiledata_fs_test)\n",
    "    mobiledata_fs_filled = pd.DataFrame(mobiledata_fs_filled, columns = [\"acc_time\", \"pid\", \"zone\", \n",
    "                                                                     \"status\", \"xcor\", \"ycor\", \"tpid\", \"tzone\", \n",
    "                                                                     \"radius\", \"txcor\", \"tycor\"])\n",
    "# add the day and minute information back\n",
    "    mobiledata_daymin = mobiledata_fs.ix[:,[0, 1, 2, 3, 8]]\n",
    "    mobiledata_fs_filled = pd.merge(mobiledata_daymin, mobiledata_fs_filled, how = 'left', on = [\"acc_time\", \"pid\"])\n",
    "    mobiledata_cp = pd.merge(mobiledata_fs, mobiledata_fs_filled, how = 'left', on = [\"acc_time\", 'day', 'minute', \"pid\"])\n",
    "    mobiledata_fs_filled.fillna(-1, inplace=True)\n",
    "    \n",
    "    return (mobiledata_fs, mobiledata_fs_filled, mobiledata_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run_NL()\n",
    "\n",
    "mobiledata_fs, mobiledata_fs_filled, mobiledata_cp = data_input()\n",
    "#miss_imput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print mobiledata_fs_filled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobiledata_fs_filled.zone.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "mobiledata_sel = mobiledata_cp[mobiledata_cp['day'] == 0]\n",
    "ax1.scatter(mobiledata_sel['minute'], mobiledata_sel['tzone_x'])\n",
    "ax1.set_xticks(range(0, 1440, 200))\n",
    "ax1.set_yticks(range(0, 45, 15))\n",
    "ax2.plot(mobiledata_sel['minute'], mobiledata_sel['tzone_y'])\n",
    "ax2.set_xticks(range(0, 1440, 200))\n",
    "ax2.set_yticks(range(0, 45, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checking the distribution of mobile phone calls\n",
    "fig2 = plt.figure(figsize=(10,5))\n",
    "ax = fig2.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])]).hist(bins=12)\n",
    "#plt.hist(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ML\n",
    "def NL_ML(mobiledata_fs_filled):\n",
    "    # prepare for ML\n",
    "    X = mobiledata_fs_filled.drop('zone', axis=1)\n",
    "    y = mobiledata_fs_filled['zone']\n",
    "    trainX, testX, trainY, testY = train_test_split(X, y)\n",
    "    \n",
    "    ## using KNN\n",
    "    # weights might need to be changed - more weights towards the recent data?\n",
    "    # metrics? canberra, minkowski\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "    pred_knn = clf_knn.predict(testX)\n",
    "    #print roc_auc_score(testY, pred_knn)\n",
    "    knn_PR = precision_score(testY, pred_knn, average='micro')\n",
    "    knn_CM = confusion_matrix(testY, pred_knn)\n",
    "    \n",
    "    ## using ridge regression\n",
    "    clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "    pred_rdg = clf_rdg.predict(testX)\n",
    "    rdg_PR = precision_score(testY, pred_rdg, average='micro')  \n",
    "    rdg_CM = confusion_matrix(testY, pred_rdg)\n",
    "    \n",
    "    ## using RF\n",
    "    clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "    clf_rf.fit(trainX, trainY)\n",
    "    pred_rf = clf_rf.predict(testX)\n",
    "    rf_PR = precision_score(testY, pred_rf, average='micro')  \n",
    "    rf_CM = confusion_matrix(testY, pred_rf)\n",
    "    \n",
    "    ## using GBDT\n",
    "    clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "    clf_gb.fit(trainX, trainY)\n",
    "    pred_gb = clf_gb.predict(testX)\n",
    "    gb_PR = precision_score(testY, pred_gb, average='micro')  \n",
    "    gb_CM = confusion_matrix(testY, pred_gb)\n",
    "    \n",
    "    return (knn_PR, knn_CM, rdg_PR, rdg_CM, rf_PR, rf_CM, gb_PR, gb_CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig3 = plt.figure(figsize=(10,5))\n",
    "ax = fig3.add_subplot(1, 1, 1)\n",
    "#ax.plot(mobiledata_fs['minute'][pd.notnull(mobiledata_fs['tzone'])], 'k.')\n",
    "#ax.plot(pd.notnull(mobiledata_fs['tzone']), 'k.')\n",
    "#ax.set_yticks([0, 1, 2])\n",
    "ax.plot(knn_all_PR, label='NN', color=\"g\")\n",
    "ax.plot(rdg_all_PR, label='Ridge', color=\"b\")\n",
    "ax.plot(rf_all_PR, label='RF', color=\"y\")\n",
    "ax.plot(gb_all_PR, label='GBDT', color=\"r\")\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax.set_ylabel(\"precision\")\n",
    "ax.set_xlabel(\"Call Frequency - number of calls per day\")\n",
    "ax.set_xticklabels(range(1,10))\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_all_PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = NL_ML(comdata_fs_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print gb_all_PR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare for ML\n",
    "\n",
    "X = mobiledata_fs_filled.drop('zone', axis=1)\n",
    "y = mobiledata_fs_filled['zone']\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using KNN\n",
    "# weights might need to be changed - more weights towards the recent data?\n",
    "# metrics? canberra, minkowski\n",
    "\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=1).fit(trainX, trainY)\n",
    "pred_knn = clf_knn.predict(testX)\n",
    "#print roc_auc_score(testY, pred_knn)\n",
    "print precision_score(testY, pred_knn, average='micro')  \n",
    "print recall_score(testY, pred_knn, average='micro')\n",
    "confusion_matrix(testY, pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_knn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testX, pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using Ridge\n",
    "\n",
    "clf_rdg = RidgeClassifier(alpha=1).fit(trainX,trainY)\n",
    "pred_rdg = clf_rdg.predict(testX)\n",
    "print precision_score(testY, pred_rdg, average='micro')  \n",
    "print recall_score(testY, pred_rdg, average='micro')\n",
    "confusion_matrix(testY, pred_rdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## using Lasso\n",
    "\n",
    "clf_las = Lasso(alpha=1).fit(trainX,trainY)\n",
    "pred_las = clf_las.predict(testX)\n",
    "print precision_score(testY, pred_las, average='micro')  \n",
    "print recall_score(testY, pred_las, average='micro')\n",
    "confusion_matrix(testY, pred_las)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using RF\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "clf_rf.fit(trainX, trainY)\n",
    "pred_rf = clf_rf.predict(testX)\n",
    "print precision_score(testY, pred_rf, average='micro')  \n",
    "print recall_score(testY, pred_rf, average='micro')\n",
    "confusion_matrix(testY, pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## using GBDT\n",
    "\n",
    "clf_gb = GradientBoostingClassifier(n_estimators=100, max_depth=3)\n",
    "clf_gb.fit(trainX, trainY)\n",
    "pred_gb = clf_gb.predict(testX)\n",
    "print precision_score(testY, pred_gb, average='micro')  \n",
    "print recall_score(testY, pred_gb, average='micro')\n",
    "confusion_matrix(testY, pred_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict_NNa(trainX, trainY, testX, activate_functions, hidden_layers, reg_lambda):\n",
    "    X_ = theano.shared(trainX, name='X')\n",
    "    y_ = theano.shared(trainY, name='y')\n",
    "    param = T.vector()\n",
    "    dim = X.shape[1]\n",
    "    num_examples = X.shape[0]\n",
    "    dim_y = len(np.unique(trainY))\n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        #going through hidden layers\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        N = dim_previous * dim_y\n",
    "        v_ = parameter[n_previous:n_previous + N].reshape((dim_previous, dim_y))\n",
    "        output = h.dot(v_)\n",
    "        n_previous += N\n",
    "        return T.nnet.softmax(output), n_previous\n",
    "\n",
    "    y_hat, dim_W = activation(X_, param)\n",
    "    # optimize\n",
    "    #regularisation\n",
    "    loss_reg = 1./num_examples * reg_lambda/2 * T.sum(T.sqr(param)) \n",
    "    #cost\n",
    "    loss = T.nnet.categorical_crossentropy(y_hat, y_).mean() + loss_reg\n",
    "    loss_function = theano.function([param], loss)\n",
    "    loss_grad = theano.function([param], theano.grad(loss, param))\n",
    "    \n",
    "    result = minimize(loss_function, jac=loss_grad, x0=np.random.normal(size=activation(X_, param)[1]))\n",
    "    optimal_params = result['x']\n",
    "#    print result\n",
    "    forward_prop = theano.function([param], y_hat)\n",
    "    theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, outfile='img/nn-theano-forward_prop.png', format='png')\n",
    "    SVG(theano.printing.pydotprint(forward_prop, var_with_name_simple=True, compact=True, return_image=True, format='svg'))\n",
    "\n",
    "    # predict data\n",
    "    data = T.matrix()\n",
    "#    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    prediction = T.argmax(activation(data, param)[0], axis=1)\n",
    "    predict = theano.function([data, param], prediction)\n",
    "    return predict(testX, optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define activation function\n",
    "#sigmoid (which we used, T.nnet.sigmoid)\n",
    "#leaky ReLU (defined below)\n",
    "#softplus (T.nnet.softplus)\n",
    "\n",
    "def LeakyReLU(x):\n",
    "    return T.switch(x > 0, x, 0.5 * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from IPython.display import Image\n",
    "X, y = make_moons(n_samples=20000, noise=0.1)\n",
    "trainX, testX, trainY, testY = train_test_split(X, y, train_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = traintestsplit_tot[0, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.vstack((trainX.values, testX.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_ = T.matrix('X') # matrix of doubles\n",
    "# y_ = T.lvector('y') # vector of int64\n",
    "\n",
    "pred = fit_predict_NN(trainX.values, trainY_.values, testX.values, [T.tanh]*2, [10, 5], 0.5)\n",
    "#roc_auc_score(testY_, pred)\n",
    "print precision_score(testY_, pred, average='micro')\n",
    "print recall_score(testY_, pred, average='micro')\n",
    "pred_ = pred.copy()\n",
    "pred_[pred==0] = 15\n",
    "pred_[pred==1] = 25\n",
    "pred_[pred==2] = 35\n",
    "pred_[pred==3] = 45\n",
    "pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "#train_X, train_y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "X, y = sklearn.datasets.make_blobs(200, n_features=2, centers=3)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "# train_X = train_X.astype(np.float32)\n",
    "# train_y = train_y.astype(np.int32)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fit_predict_NN(train_X, train_y, test_X, [T.tanh], [100])\n",
    "print precision_score(test_y, pred, average='micro')\n",
    "print recall_score(test_y, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "aa = trainY.as_matrix()\n",
    "bb = aa.reshape((aa.shape[0], 1))\n",
    "trainY_ = MultiLabelBinarizer().fit_transform(bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainY_ = np.tile(np.zeros((1,4)), (trainY.shape[0],1))\n",
    "trainY_[trainY == 15] = [[1, 0, 0, 0]]\n",
    "# trainY_[trainY == 25] = [0, 1, 0, 0]\n",
    "# trainY_[trainY == 35] = [0, 0, 1, 0]\n",
    "# trainY_[trainY == 45] = [0, 0, 0, 1]\n",
    "trainY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "A = np.random.rand(1000,10000).astype(theano.config.floatX)\n",
    "B = np.random.rand(10000,1000).astype(theano.config.floatX)\n",
    "np_start = time.time()\n",
    "AB = A.dot(B)\n",
    "np_end = time.time()\n",
    "X,Y = theano.tensor.matrices('XY')\n",
    "mf = theano.function([X,Y],X.dot(Y))\n",
    "t_start = time.time()\n",
    "tAB = mf(A,B)\n",
    "t_end = time.time()\n",
    "print(\"NP time: %f[s], theano time: %f[s] (times should be close when run on CPU!)\" %(\n",
    "                                           np_end-np_start, t_end-t_start))\n",
    "print(\"Result difference: %f\" % (np.abs(AB-tAB).max(), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
